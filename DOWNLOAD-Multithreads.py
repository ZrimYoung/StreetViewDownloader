import os
import requests
import pandas as pd
from PIL import Image
from io import BytesIO
from time import sleep
from tqdm import tqdm, trange
from configparser import ConfigParser
import logging
import traceback
import concurrent.futures # å¯¼å…¥å¤šçº¿ç¨‹æ¨¡å—
import threading # ç”¨äº tqdm çš„é”

# ===== è®¾ç½®è¯¦ç»†æ—¥å¿—è®°å½• =====
def setup_logger(log_path):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger_obj = logging.getLogger('detailed_downloader') # ä½¿ç”¨ logger_obj é¿å…ä¸å¤–å±‚ logger å˜é‡åå†²çª
    logger_obj.setLevel(logging.DEBUG)

    if not logger_obj.handlers:
        fh = logging.FileHandler(log_path, encoding='utf-8')
        fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
        fh.setFormatter(formatter)
        logger_obj.addHandler(fh)
    return logger_obj

# ===== å•ä¸ªç‚¹ä½å¤„ç†å‡½æ•° (ç”¨äºå¤šçº¿ç¨‹) =====
def process_single_point(point_data_tuple, pano_id_str, api_key_str, session_token_str, zoom_int, tile_cols_int, tile_rows_int, tile_size_int, sleeptime_float, save_dir_str, logger_obj, thread_local_storage):
    """
    å¤„ç†å•ä¸ªç‚¹ä½çš„å›¾åƒä¸‹è½½å’Œæ‹¼æ¥ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å¤„ç†ç»“æœçš„å­—å…¸ã€‚
    """
    current_point_id_str = str(point_data_tuple.ID) # ä» namedtuple è·å– ID
    # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å­˜å‚¨æ¥ç®¡ç† trange çš„è¾“å‡ºï¼Œé¿å…äº¤é”™
    # æˆ–è€…ç›´æ¥åœ¨ä¸»çº¿ç¨‹çš„tqdmä¸­æ›´æ–°ï¼Œè¿™é‡Œæš‚æ—¶ä¿ç•™ trangeï¼Œä½†è¾“å‡ºå¯èƒ½äº¤é”™
    # å¦‚æœ trange è¾“å‡ºæ··ä¹±ï¼Œå¯ä»¥è€ƒè™‘ç§»é™¤æˆ–å¯»æ‰¾æ›´é«˜çº§çš„tqdmçº¿ç¨‹å®‰å…¨ç”¨æ³•

    logger_obj.info(f"çº¿ç¨‹ {threading.get_ident()}: å¼€å§‹å¤„ç†ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str}")

    if not pano_id_str: # pano_id ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²
        logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str} æœªæ‰¾åˆ° PanoIDã€‚")
        return {"status": "failure", "id": current_point_id_str, "reason": "No panoId"}

    panorama = Image.new('RGB', (tile_size_int * tile_cols_int, tile_size_int * tile_rows_int))
    missing_tiles = 0
    total_tiles = tile_cols_int * tile_rows_int
    logger_obj.debug(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - åˆ›å»ºç©ºç™½å›¾åƒï¼Œé¢„æœŸæ€»ç“¦ç‰‡æ•°: {total_tiles}")

    # trange åœ¨å¤šçº¿ç¨‹ä¸­ç›´æ¥ä½¿ç”¨ï¼Œå…¶è¾“å‡ºå¯èƒ½ä¼šäº¤é”™ã€‚
    # å¦‚æœéœ€è¦æ›´å¹²å‡€çš„è¾“å‡ºï¼Œå¯ä»¥è€ƒè™‘åœ¨ process_single_point å†…éƒ¨ä¸ä½¿ç”¨ trangeï¼Œ
    # æˆ–è€…ä½¿ç”¨ä¸€ä¸ªå…¨å±€çš„ tqdm å¯¹è±¡åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°å­ä»»åŠ¡è¿›åº¦ã€‚
    # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæš‚æ—¶ä¿ç•™ trangeã€‚
    for x in trange(tile_cols_int, desc=f"æ‹¼æ¥ {current_point_id_str} (çº¿ç¨‹ {threading.get_ident()})", leave=False, position=threading.get_ident() % 10): # å°è¯•ä½¿ç”¨ position åˆ†æ•£è¿›åº¦æ¡
        for y in range(tile_rows_int):
            tile_url = (
                f"https://tile.googleapis.com/v1/streetview/tiles/{zoom_int}/{x}/{y}"
                f"?session={session_token_str}&key={api_key_str}&panoId={pano_id_str}"
            )
            logger_obj.debug(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - è¯·æ±‚ç“¦ç‰‡ URL: {tile_url}")
            
            try:
                tile_resp = requests.get(tile_url, timeout=10) 
                if tile_resp.status_code == 200:
                    tile_img = Image.open(BytesIO(tile_resp.content))
                    panorama.paste(tile_img, (x * tile_size_int, y * tile_size_int))
                else:
                    missing_tiles += 1
                    logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - ç“¦ç‰‡ ({x},{y}) ä¸‹è½½å¤±è´¥ã€‚çŠ¶æ€ç : {tile_resp.status_code}")
            except requests.exceptions.RequestException as req_e:
                missing_tiles += 1
                logger_obj.error(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - ç“¦ç‰‡ ({x},{y}) è¯·æ±‚å¼‚å¸¸: {req_e}")
            
            sleep(sleeptime_float) 

    if missing_tiles == total_tiles:
        logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - æ‰€æœ‰ç“¦ç‰‡å‡ç¼ºå¤±ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        return {"status": "failure", "id": current_point_id_str, "reason": "All tiles missing"}

    filename = f"{current_point_id_str}_{pano_id_str}.jpg"
    filepath = os.path.join(save_dir_str, filename)
    try:
        panorama.save(filepath)
        logger_obj.info(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - å›¾åƒæˆåŠŸä¿å­˜è‡³: {filepath}")
        return {"status": "success", "id": current_point_id_str, "panoId": pano_id_str, "file": filename}
    except Exception as e_save:
        logger_obj.error(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - ä¿å­˜å›¾åƒå¤±è´¥: {e_save}", exc_info=True)
        return {"status": "failure", "id": current_point_id_str, "reason": str(e_save)}


if __name__ == "__main__":
    logger = None 
    # tqdm å…¨å±€é”ï¼Œç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å®‰å…¨è¾“å‡º
    tqdm.set_lock(threading.RLock())
    thread_local_storage = threading.local() # ç”¨äºçº¿ç¨‹ç‰¹å®šçš„æ•°æ®ï¼Œå¦‚æœéœ€è¦çš„è¯

    try:
        config = ConfigParser()
        if not os.path.exists('configuration.ini'):
            print("âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ 'configuration.ini' æœªæ‰¾åˆ°ã€‚è¯·åˆ›å»ºè¯¥æ–‡ä»¶ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        with open('configuration.ini', 'r', encoding='utf-8') as f:
            config.read_file(f)

        CSV_PATH = config['PATHS']['CSV_PATH']
        API_KEY_PATH = config['PATHS']['API_KEY_PATH']
        SAVE_DIR = config['PATHS']['SAVE_DIR']
        LOG_PATH = config['PATHS']['LOG_PATH']
        FAIL_LOG_PATH = config['PATHS']['FAIL_LOG_PATH']
        DETAILED_LOG_PATH = config['PATHS']['DETAILED_LOG_PATH']

        logger = setup_logger(DETAILED_LOG_PATH)
        logger.info("ç¨‹åºå¼€å§‹è¿è¡Œã€‚")
        logger.info(f"é…ç½®æ–‡ä»¶ 'configuration.ini' åŠ è½½æˆåŠŸã€‚")

        BATCH_SIZE = int(config['PARAMS']['BATCH_SIZE'])
        NUM_BATCHES = int(config['PARAMS']['NUM_BATCHES'])
        RETRY_FAILED_POINTS = config.getboolean('PARAMS', 'RETRY_FAILED_POINTS', fallback=False)
        MAX_POINT_WORKERS = config.getint('PARAMS', 'MAX_POINT_WORKERS', fallback=5) # è¯»å–çº¿ç¨‹æ•°é…ç½®
        logger.info(f"å‚æ•°åŠ è½½ï¼šBATCH_SIZE={BATCH_SIZE}, NUM_BATCHES={NUM_BATCHES}, RETRY_FAILED_POINTS={RETRY_FAILED_POINTS}, MAX_POINT_WORKERS={MAX_POINT_WORKERS}")

        ZOOM = int(config['TILES']['ZOOM'])
        TILE_SIZE = int(config['TILES']['TILE_SIZE'])
        TILE_COLS = int(config['TILES']['TILE_COLS'])
        TILE_ROWS = int(config['TILES']['TILE_ROWS'])
        SLEEPTIME = float(config['TILES']['SLEEPTIME'])
        logger.info(f"å‚æ•°åŠ è½½ï¼šZOOM={ZOOM}, TILE_SIZE={TILE_SIZE}, TILE_COLS={TILE_COLS}, TILE_ROWS={TILE_ROWS}, SLEEPTIME={SLEEPTIME}")

        print(f"ğŸ”§ å½“å‰è®¾ç½®ï¼šZOOM={ZOOM}, TILE_SIZE={TILE_SIZE}, TILE_COLS={TILE_COLS}, TILE_ROWS={TILE_ROWS}")
        print(f"ğŸ–¼ï¸ é¢„æœŸæ‹¼æ¥å›¾åƒå¤§å°ï¼š{TILE_COLS * TILE_SIZE} x {TILE_ROWS * TILE_SIZE} åƒç´ ")
        print(f"ğŸ”„ é‡è¯•å¤±è´¥ç‚¹ä½æ¨¡å¼: {'å¼€å¯' if RETRY_FAILED_POINTS else 'å…³é—­'}")
        print(f"ğŸ§µ ç‚¹ä½å¤„ç†å¹¶å‘çº¿ç¨‹æ•°: {MAX_POINT_WORKERS}")
        logger.info(f"ç‚¹ä½å¤„ç†å¹¶å‘çº¿ç¨‹æ•°: {MAX_POINT_WORKERS}")


        os.makedirs(SAVE_DIR, exist_ok=True)
        logger.info(f"ä¿å­˜ç›®å½• '{SAVE_DIR}' å·²ç¡®è®¤/åˆ›å»ºã€‚")
        if not os.path.exists(API_KEY_PATH):
            logger.error(f"API Key æ–‡ä»¶ '{API_KEY_PATH}' æœªæ‰¾åˆ°ã€‚")
            print(f"âŒ é”™è¯¯ï¼šAPI Key æ–‡ä»¶ '{API_KEY_PATH}' æœªæ‰¾åˆ°ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        with open(API_KEY_PATH, 'r') as f:
            API_KEY = f.readline().strip()
        logger.info(f"API Key ä» '{API_KEY_PATH}' åŠ è½½æˆåŠŸã€‚")

        downloaded_ids = set()
        if os.path.exists(LOG_PATH):
            try:
                log_content_df = pd.read_csv(LOG_PATH)
                if 'ID' in log_content_df.columns:
                    downloaded_ids = set(log_content_df['ID'].dropna().astype(str))
                    logger.info(f"ä» '{LOG_PATH}' åŠ è½½å·²æˆåŠŸä¸‹è½½ {len(downloaded_ids)} ä¸ªIDã€‚")
                else: logger.warning(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            except pd.errors.EmptyDataError: logger.warning(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' ä¸ºç©ºã€‚")
            except Exception as e_log: logger.error(f"è¯»å–æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_log}")
        else:
            pd.DataFrame(columns=['ID']).to_csv(LOG_PATH, index=False)
            logger.info(f"'{LOG_PATH}' ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç©ºçš„æˆåŠŸæ—¥å¿—æ–‡ä»¶ã€‚")

        failed_df_list = [] # ç”¨äºæ”¶é›†å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•
        # åˆå§‹æ—¶ä»æ–‡ä»¶åŠ è½½å†å²å¤±è´¥è®°å½•ï¼Œç”¨äºæ„å»º ids_to_skip_processing
        initial_failed_ids_from_file = set()
        if os.path.exists(FAIL_LOG_PATH):
            try:
                temp_failed_df = pd.read_csv(FAIL_LOG_PATH)
                if not temp_failed_df.empty and 'ID' in temp_failed_df.columns:
                    initial_failed_ids_from_file = set(temp_failed_df['ID'].dropna().astype(str).unique())
                logger.info(f"ä» '{FAIL_LOG_PATH}' åˆå§‹åŠ è½½ {len(initial_failed_ids_from_file)} ä¸ªå”¯ä¸€å¤±è´¥IDï¼ˆç”¨äºè·³è¿‡é€»è¾‘ï¼‰ã€‚")
            except pd.errors.EmptyDataError: logger.warning(f"å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' ä¸ºç©ºã€‚")
            except Exception as e_fail_log: logger.error(f"è¯»å–å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_fail_log}")
        else:
            pd.DataFrame(columns=['ID', 'Reason']).to_csv(FAIL_LOG_PATH, index=False)
            logger.info(f"'{FAIL_LOG_PATH}' ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç©ºçš„å¤±è´¥æ—¥å¿—æ–‡ä»¶ã€‚")

        ids_to_skip_processing = set(downloaded_ids)
        if not RETRY_FAILED_POINTS:
            logger.info("é‡è¯•æ¨¡å¼å…³é—­ï¼Œå°†é¢å¤–è·³è¿‡ä¹‹å‰å¤±è´¥è®°å½•ä¸­çš„IDã€‚")
            if initial_failed_ids_from_file:
                ids_to_skip_processing.update(initial_failed_ids_from_file)
                logger.info(f"å·²å°† {len(initial_failed_ids_from_file)} ä¸ªä¹‹å‰è®°å½•çš„å¤±è´¥IDæ·»åŠ åˆ°è·³è¿‡åˆ—è¡¨ã€‚")
        else:
            logger.info("é‡è¯•æ¨¡å¼å¼€å¯ï¼Œå°†ä»…è·³è¿‡å·²æˆåŠŸä¸‹è½½çš„IDã€‚")
        logger.info(f"æ€»è®¡å°†æ˜ç¡®è·³è¿‡ {len(ids_to_skip_processing)} ä¸ªIDã€‚")

        logger.info("å°è¯•åˆ›å»ºè¡—æ™¯ä¼šè¯ Token...")
        session_payload = {"mapType": "streetview", "language": "en-US", "region": "US"}
        session_response = requests.post(f"https://tile.googleapis.com/v1/createSession?key={API_KEY}", headers={"Content-Type": "application/json"}, json=session_payload, timeout=15)
        
        if session_response.status_code == 200:
            SESSION_TOKEN = session_response.json().get("session")
            if not SESSION_TOKEN:
                logger.error(f"æ— æ³•è·å– session tokenï¼Œå“åº”å†…å®¹ï¼š{session_response.text}")
                print(f"âŒ æ— æ³•è·å– session tokenï¼Œå“åº”å†…å®¹ï¼š\n{session_response.text}")
                raise Exception("æ— æ³•è·å– session token")
            logger.info(f"æˆåŠŸè·å– Session Token: {SESSION_TOKEN[:10]}...")
        else:
            logger.error(f"åˆ›å»º Session Token è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {session_response.status_code}, å“åº”: {session_response.text}")
            print(f"âŒ åˆ›å»º Session Token è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {session_response.status_code}\n{session_response.text}")
            raise Exception(f"åˆ›å»º Session Token è¯·æ±‚å¤±è´¥")

        if not os.path.exists(CSV_PATH):
            logger.error(f"ç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' æœªæ‰¾åˆ°ã€‚")
            print(f"âŒ é”™è¯¯ï¼šç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' æœªæ‰¾åˆ°ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        all_df = pd.read_csv(CSV_PATH)
        if 'ID' not in all_df.columns:
            logger.error(f"ç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            print(f"âŒ é”™è¯¯ï¼šç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        all_df['ID'] = all_df['ID'].astype(str)
        logger.info(f"ä» '{CSV_PATH}' åŠ è½½ {len(all_df)} ä¸ªæ€»ç‚¹ä½ã€‚")
        
        current_run_log_list = [] # å­˜å‚¨å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•å­—å…¸

        for batch_num in range(NUM_BATCHES):
            logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{NUM_BATCHES}")
            
            # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¼€å§‹æ—¶ï¼ŒåŸºäºæœ€æ–°çš„ ids_to_skip_processing ç­›é€‰
            # (ids_to_skip_processing ä¼šåœ¨æ‰¹æ¬¡å†…å¤„ç†ç‚¹ååŠ¨æ€æ›´æ–°)
            current_processing_df = all_df[~all_df['ID'].isin(list(ids_to_skip_processing))].head(BATCH_SIZE) # list() for safety with some pandas versions
            
            if current_processing_df.empty:
                print("ğŸ‰ æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç‚¹ä½å·²å¤„ç†å®Œæ¯•ï¼Œæ— éœ€å†è¿è¡Œæ›´å¤šæ‰¹æ¬¡ã€‚")
                logger.info("æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç‚¹ä½å·²å¤„ç†å®Œæ¯•ï¼Œæ— éœ€å†è¿è¡Œæ›´å¤šæ‰¹æ¬¡ã€‚")
                break
            
            print(f"\nğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {batch_num + 1}/{NUM_BATCHES} æ‰¹ï¼Œå…± {len(current_processing_df)} ä¸ªç‚¹ä½...")
            logger.info(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šå¾…å¤„ç†ç‚¹ä½æ•° {len(current_processing_df)}")

            if not all(col in current_processing_df.columns for col in ['Lat', 'Lng']):
                logger.error("å½“å‰æ‰¹æ¬¡ç‚¹ä½æ•°æ®ä¸­ç¼ºå°‘ 'Lat' æˆ– 'Lng' åˆ—ã€‚è¯·æ£€æŸ¥CSVæ–‡ä»¶ã€‚")
                print("âŒ é”™è¯¯ï¼šå½“å‰æ‰¹æ¬¡ç‚¹ä½æ•°æ®ä¸­ç¼ºå°‘ 'Lat' æˆ– 'Lng' åˆ—ã€‚")
                continue

            locations = [{"lat": row["Lat"], "lng": row["Lng"]} for _, row in current_processing_df.iterrows()]
            logger.debug(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè¯·æ±‚ PanoIDs çš„åœ°ç‚¹ (å‰5ä¸ª): {locations[:5]}")
            
            panoid_url = f"https://tile.googleapis.com/v1/streetview/panoIds?session={SESSION_TOKEN}&key={API_KEY}"
            pano_ids_data = []
            try:
                response_pano_ids = requests.post(panoid_url, json={"locations": locations, "radius": 50}, timeout=20)
                if response_pano_ids.status_code == 200:
                    try:
                        pano_ids_data = response_pano_ids.json().get("panoIds", [])
                        logger.info(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šæˆåŠŸè·å– PanoIDs å“åº”ã€‚æ•°é‡: {len(pano_ids_data)}")
                        logger.debug(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè·å–åˆ°çš„ PanoIDs (éƒ¨åˆ†): {pano_ids_data[:5]}")
                    except requests.exceptions.JSONDecodeError:
                        logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè·å– PanoIDs æˆåŠŸï¼Œä½†JSONè§£æå¤±è´¥ã€‚å“åº”æ–‡æœ¬: {response_pano_ids.text}")
                else:
                    logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè·å– PanoIDs è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {response_pano_ids.status_code}, å“åº”: {response_pano_ids.text}")
            except requests.exceptions.RequestException as req_e_pano:
                logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè·å– PanoIDs è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {req_e_pano}")
            
            print("ğŸ“ å·²è·å– panoIds") 
            results_this_batch_filenames = [] 

            # ç¡®ä¿ pano_ids_data é•¿åº¦ä¸ current_processing_df åŒ¹é…
            # å¦‚æœAPIè¿”å›çš„pano_ids_dataæ¯”locationsçŸ­ï¼Œéœ€è¦å¤„ç†å¯¹é½é—®é¢˜
            # é€šå¸¸APIä¼šè¿”å›ç­‰é•¿åˆ—è¡¨ï¼ŒåŒ…å«nullã€‚å¦‚æœä¸æ˜¯ï¼Œè¿™é‡Œéœ€è¦é¢å¤–é€»è¾‘ã€‚
            # å‡è®¾pano_ids_dataä¸locationsç­‰é•¿
            if len(pano_ids_data) != len(locations):
                logger.warning(f"æ‰¹æ¬¡ {batch_num + 1}: PanoIDæ•°é‡({len(pano_ids_data)})ä¸åœ°ç‚¹æ•°({len(locations)})ä¸åŒ¹é…ã€‚å°†å°è¯•æŒ‰åœ°ç‚¹æ•°è¿­ä»£ï¼ŒPanoIDä¸è¶³å¤„ä¼šä¸ºNoneã€‚")
                # è¡¥é½ pano_ids_data åˆ° locations çš„é•¿åº¦ï¼Œç”¨ None å¡«å……
                pano_ids_data.extend([None] * (len(locations) - len(pano_ids_data)))


            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_POINT_WORKERS) as executor:
                future_to_point = {}
                for i, point_row_tuple in enumerate(current_processing_df.itertuples(index=False)):
                    # ä» pano_ids_data ä¸­è·å–å¯¹åº”çš„ pano_idï¼Œæ³¨æ„ç´¢å¼•å®‰å…¨
                    current_pano_id = pano_ids_data[i] if i < len(pano_ids_data) else None
                    
                    # æäº¤ä»»åŠ¡
                    future = executor.submit(process_single_point, 
                                             point_row_tuple, current_pano_id, API_KEY, SESSION_TOKEN, 
                                             ZOOM, TILE_COLS, TILE_ROWS, TILE_SIZE, 
                                             SLEEPTIME, SAVE_DIR, logger, thread_local_storage)
                    future_to_point[future] = str(point_row_tuple.ID) # ä½¿ç”¨åŸå§‹IDä½œä¸ºé”®

                # ä½¿ç”¨ tqdm åŒ…è£… concurrent.futures.as_completed æ¥æ˜¾ç¤ºæ€»ä½“è¿›åº¦
                for future in tqdm(concurrent.futures.as_completed(future_to_point), total=len(future_to_point), desc=f"å¤„ç†æ‰¹æ¬¡ {batch_num + 1} ç‚¹ä½"):
                    point_id_processed = future_to_point[future]
                    try:
                        result = future.result() # è·å–çº¿ç¨‹çš„è¿”å›ç»“æœ
                        if result['status'] == 'success':
                            results_this_batch_filenames.append({"ID": result['id'], "panoId": result['panoId'], "file": result['file']})
                            current_run_log_list.append({"ID": result['id']})
                            ids_to_skip_processing.add(result['id']) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆæˆåŠŸï¼‰
                        else: # status == 'failure'
                            failed_df_list.append({"ID": result['id'], "Reason": result['reason']})
                            ids_to_skip_processing.add(result['id']) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¤±è´¥ï¼‰
                    except Exception as exc:
                        logger.error(f"ç‚¹ä½ID {point_id_processed} åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œæ—¶äº§ç”Ÿæœªæ•è·å¼‚å¸¸: {exc}", exc_info=True)
                        failed_df_list.append({"ID": point_id_processed, "Reason": f"çº¿ç¨‹å¼‚å¸¸: {exc}"})
                        ids_to_skip_processing.add(point_id_processed) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¼‚å¸¸ï¼‰
            
            # ===== æ¯æ‰¹æ¬¡ç»“æŸæ—¶ä¿å­˜ä¸€æ¬¡æœ¬æ‰¹æ¬¡äº§ç”Ÿçš„å¤±è´¥è®°å½•å’Œæ‰¹æ¬¡ç»“æœ =====
            if failed_df_list: # åªå¤„ç†å½“å‰è¿è¡Œäº§ç”Ÿçš„å¤±è´¥
                temp_batch_failed_df = pd.DataFrame(failed_df_list)
                # è¯»å–æ—§çš„å¤±è´¥æ—¥å¿—ï¼Œåˆå¹¶ï¼Œå»é‡ï¼Œç„¶åä¿å­˜
                if os.path.exists(FAIL_LOG_PATH):
                    try:
                        old_failed_df = pd.read_csv(FAIL_LOG_PATH)
                        if not old_failed_df.empty and 'ID' in old_failed_df.columns and 'Reason' in old_failed_df.columns:
                             combined_failed_df = pd.concat([old_failed_df, temp_batch_failed_df], ignore_index=True)
                        else: # æ—§æ—¥å¿—ä¸ºç©ºæˆ–æ ¼å¼ä¸å¯¹
                            combined_failed_df = temp_batch_failed_df
                    except pd.errors.EmptyDataError:
                        combined_failed_df = temp_batch_failed_df
                    except Exception as e_read_old_fail:
                        logger.error(f"è¯»å–æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_read_old_fail}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•ã€‚")
                        combined_failed_df = temp_batch_failed_df
                else: # æ—§å¤±è´¥æ—¥å¿—ä¸å­˜åœ¨
                    combined_failed_df = temp_batch_failed_df
                
                if 'ID' in combined_failed_df.columns and 'Reason' in combined_failed_df.columns:
                    combined_failed_df['ID'] = combined_failed_df['ID'].astype(str)
                    combined_failed_df.drop_duplicates(subset=['ID', 'Reason'], keep='last').to_csv(FAIL_LOG_PATH, index=False)
                failed_df_list = [] # æ¸…ç©ºå½“å‰è¿è¡Œçš„å¤±è´¥åˆ—è¡¨ï¼Œé¿å…é‡å¤æ·»åŠ 

            if results_this_batch_filenames:
                pd.DataFrame(results_this_batch_filenames).to_csv(os.path.join(SAVE_DIR, f'results_batch_{batch_num+1}.csv'), index=False)
            logger.info(f"æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆã€‚å¤±è´¥æ—¥å¿—å’Œæ‰¹æ¬¡ç»“æœå·²æ›´æ–°ã€‚")

        # ===== æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œç»Ÿä¸€æ›´æ–°æˆåŠŸæ—¥å¿— =====
        if current_run_log_list:
            new_success_df = pd.DataFrame(current_run_log_list)
            if os.path.exists(LOG_PATH):
                try:
                    old_log_df = pd.read_csv(LOG_PATH)
                    if 'ID' not in old_log_df.columns: combined_log_df = new_success_df
                    else: combined_log_df = pd.concat([old_log_df, new_success_df], ignore_index=True)
                except pd.errors.EmptyDataError: combined_log_df = new_success_df
                except Exception as e_read_old_log:
                    logger.error(f"è¯»å–æ—§æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_read_old_log}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•ã€‚")
                    combined_log_df = new_success_df
            else: combined_log_df = new_success_df

            if 'ID' in combined_log_df.columns:
                combined_log_df['ID'] = combined_log_df['ID'].astype(str)
                combined_log_df.drop_duplicates(subset=['ID'], keep='last').to_csv(LOG_PATH, index=False)
                logger.info(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' å·²æ›´æ–°ã€‚")
            else: logger.error(f"æ— æ³•æ›´æ–°æˆåŠŸæ—¥å¿—ï¼Œå› ä¸ºåˆå¹¶åçš„æ—¥å¿—ç¼ºå°‘ 'ID' åˆ—ã€‚")

        print("\nâœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚")
        logger.info("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚")

    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}") 
        if logger: logger.error("ç¨‹åºé¡¶å±‚æ•è·åˆ°æœªå¤„ç†å¼‚å¸¸ã€‚", exc_info=True) 
        else: 
            print("Logger æœªåˆå§‹åŒ–ï¼Œè¯¦ç»†é”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š")
            traceback.print_exc() 
        
        try: # å¼‚å¸¸æ—¶ä¹Ÿå°è¯•ä¿å­˜å·²æ”¶é›†çš„å¤±è´¥å’ŒæˆåŠŸè®°å½•
            if 'failed_df_list' in locals() and failed_df_list:
                # ... (ä¸æ‰¹æ¬¡ç»“æŸæ—¶ç±»ä¼¼çš„å¤±è´¥æ—¥å¿—ä¿å­˜é€»è¾‘) ...
                temp_failed_df_on_exit = pd.DataFrame(failed_df_list)
                # (ä»£ç çœç•¥ï¼Œä¸æ‰¹æ¬¡ç»“æŸæ—¶ä¿å­˜å¤±è´¥æ—¥å¿—çš„é€»è¾‘ç±»ä¼¼ï¼Œåˆå¹¶æ—§æ—¥å¿—å¹¶ä¿å­˜)
                logger.info("ç¨‹åºå¼‚å¸¸é€€å‡ºå‰ï¼Œå°è¯•ä¿å­˜å½“å‰æ”¶é›†çš„å¤±è´¥è®°å½•ã€‚")


            if 'current_run_log_list' in locals() and current_run_log_list:
                # ... (ä¸ç¨‹åºæ­£å¸¸ç»“æŸæ—¶ç±»ä¼¼çš„æˆåŠŸæ—¥å¿—ä¿å­˜é€»è¾‘) ...
                logger.info("ç¨‹åºå¼‚å¸¸é€€å‡ºå‰ï¼Œå°è¯•ä¿å­˜å½“å‰æ”¶é›†çš„æˆåŠŸè®°å½•ã€‚")

        except Exception as log_save_e:
            print(f"âŒ åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æ—¥å¿—æ—¶ä¹Ÿå‘ç”Ÿé”™è¯¯: {log_save_e}")
            if logger: logger.error(f"åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æ—¥å¿—æ—¶ä¹Ÿå‘ç”Ÿé”™è¯¯: {log_save_e}", exc_info=True)
    finally:
        if logger: logger.info("ç¨‹åºç»“æŸã€‚\n---------------------------------------\n")
        input("\næŒ‰å›è½¦é”®å…³é—­...")
