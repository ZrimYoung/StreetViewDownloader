import os
import requests
import pandas as pd
from PIL import Image
from io import BytesIO
from time import sleep
from tqdm import tqdm, trange
from configparser import ConfigParser
import logging
import traceback
import concurrent.futures # å¯¼å…¥å¤šçº¿ç¨‹æ¨¡å—
import threading # ç”¨äº tqdm çš„é”
import json # å¯¼å…¥ json æ¨¡å—ï¼Œç”¨äºè§£æAPIé”™è¯¯å“åº”
import sys # å¯¼å…¥ sys æ¨¡å—ï¼Œç”¨äºé…ç½®åŸºæœ¬æ—¥å¿—å™¨çš„è¾“å‡ºæµ

# --- é”™è¯¯ç±»å‹å¸¸é‡ ---
# å®šä¹‰å„ç§å¯èƒ½çš„é”™è¯¯ç±»å‹ï¼Œç”¨äºæ›´ç»†è‡´åœ°è®°å½•å¤±è´¥åŸå› 

# æ°¸ä¹…æ€§é”™è¯¯ï¼šè¿™äº›é”™è¯¯é€šå¸¸æŒ‡ç¤ºæ•°æ®æœ¬èº«çš„é—®é¢˜ï¼Œæˆ–éœ€è¦äººå·¥å¹²é¢„ï¼Œåœ¨é‡è¯•æ¨¡å¼ä¸‹ä¼šè¢«è·³è¿‡ã€‚
ERROR_TYPE_NO_PANOID_FOUND = "NO_PANOID_FOUND" # åæ ‡å¤„æ²¡æœ‰æ‰¾åˆ°PanoId (APIè¿”å›ç©ºå­—ç¬¦ä¸²)

# å¯é‡è¯•é”™è¯¯ï¼šè¿™äº›é”™è¯¯é€šå¸¸æ˜¯æš‚æ—¶æ€§çš„ï¼Œæˆ–è€…åœ¨é‡è¯•æ¨¡å¼ä¸‹æˆ‘ä»¬å¸Œæœ›å†æ¬¡å°è¯•ã€‚
ERROR_TYPE_ALL_TILES_MISSING = "ALL_TILES_MISSING_AFTER_RETRIES" # æ‰€æœ‰ç“¦ç‰‡ä¸‹è½½å¤±è´¥ï¼ˆå¯èƒ½å› 404æˆ–æŒç»­ç½‘ç»œé—®é¢˜ï¼‰
ERROR_TYPE_API_AUTH_FORBIDDEN = "API_AUTH_FORBIDDEN" # 401/403 æƒé™æˆ–é…é¢é—®é¢˜ (é€šå¸¸è‡´å‘½ï¼Œä½†æ ¹æ®æ–°ç­–ç•¥ä¼šé‡è¯•)
ERROR_TYPE_API_BAD_REQUEST = "API_BAD_REQUEST" # 400 è¯·æ±‚å‚æ•°é”™è¯¯ (é€šå¸¸æ˜¯å®¢æˆ·ç«¯é—®é¢˜ï¼Œä½†æ ¹æ®æ–°ç­–ç•¥ä¼šé‡è¯•)
ERROR_TYPE_INTERNAL_PROCESSING_ERROR = "INTERNAL_PROCESSING_ERROR" # å†…éƒ¨å¤„ç†é”™è¯¯ï¼ˆå¦‚PILå›¾åƒå¤„ç†ï¼Œæ–‡ä»¶ä¿å­˜ï¼‰
ERROR_TYPE_NETWORK_TIMEOUT = "NETWORK_TIMEOUT" # ç½‘ç»œè¯·æ±‚è¶…æ—¶
ERROR_TYPE_NETWORK_CONNECTION_ERROR = "NETWORK_CONNECTION_ERROR" # ç½‘ç»œè¿æ¥é”™è¯¯
ERROR_TYPE_API_RATE_LIMIT = "API_RATE_LIMIT" # 429 é€Ÿç‡é™åˆ¶ (éœ€è¦æŒ‡æ•°é€€é¿)
ERROR_TYPE_API_SERVER_ERROR = "API_SERVER_ERROR" # 5xx æœåŠ¡å™¨ç«¯é”™è¯¯
ERROR_TYPE_PANOID_JSON_PARSE_ERROR = "PANOID_JSON_PARSE_ERROR" # PanoIdå“åº”JSONè§£æå¤±è´¥
ERROR_TYPE_UNCLASSIFIED_HTTP_STATUS = "UNCLASSIFIED_HTTP_STATUS" # é‡åˆ°æœªçŸ¥HTTPçŠ¶æ€ç 
ERROR_TYPE_UNCLASSIFIED_REQUEST_ERROR = "UNCLASSIFIED_REQUEST_ERROR" # å…¶ä»–requests.exceptions.RequestException
ERROR_TYPE_GENERAL_EXCEPTION = "GENERAL_EXCEPTION" # æ•è·åˆ°çš„ä¸ç¬¦åˆä¸Šè¿°åˆ†ç±»çš„é€šç”¨å¼‚å¸¸

# èšåˆçš„æ°¸ä¹…æ€§é”™è¯¯ç±»å‹é›†åˆï¼Œç”¨äºæ§åˆ¶è·³è¿‡é€»è¾‘ã€‚
# æ ¹æ®ä½ çš„è¦æ±‚ï¼Œåªæœ‰ NO_PANOID_FOUND ä¼šè¢«æ°¸ä¹…è·³è¿‡ã€‚
PERMANENT_SKIP_ERROR_TYPES = {
    ERROR_TYPE_NO_PANOID_FOUND
}

# ===== è®¾ç½®è¯¦ç»†æ—¥å¿—è®°å½•å‡½æ•° =====
def setup_logger(log_file_path, console_output_level=logging.WARNING, file_output_level=logging.DEBUG):
    """
    é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ã€‚
    è¯¥æ—¥å¿—å™¨ä¼šå°†æ‰€æœ‰æŒ‡å®šçº§åˆ«çš„æ—¥å¿—å†™å…¥æ–‡ä»¶ï¼Œå¹¶å°†æŒ‡å®šçº§åˆ«åŠä»¥ä¸Šçš„æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚

    Args:
        log_file_path (str): æ—¥å¿—æ–‡ä»¶ä¿å­˜çš„å®Œæ•´è·¯å¾„ã€‚
        console_output_level (int): æ§åˆ¶å°è¾“å‡ºçš„æœ€ä½æ—¥å¿—çº§åˆ« (ä¾‹å¦‚ logging.WARNING)ã€‚
        file_output_level (int): æ–‡ä»¶è¾“å‡ºçš„æœ€ä½æ—¥å¿—çº§åˆ« (ä¾‹å¦‚ logging.DEBUG)ã€‚
    Returns:
        logging.Logger: é…ç½®å¥½çš„æ—¥å¿—å™¨å¯¹è±¡ã€‚
    """
    logger_obj = logging.getLogger('detailed_downloader') # è·å–åä¸º 'detailed_downloader' çš„æ—¥å¿—å™¨
    logger_obj.setLevel(file_output_level) # è®¾ç½®æ—¥å¿—å™¨æ•è·çš„æœ€ä½çº§åˆ«ï¼Œé€šå¸¸ä¸ºæ–‡ä»¶è¾“å‡ºçš„æœ€ä½çº§åˆ«

    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤æ·»åŠ ï¼ˆå¦‚æœå‡½æ•°è¢«å¤šæ¬¡è°ƒç”¨ï¼‰
    # è¿­ä»£ä¸€ä¸ªåˆ—è¡¨çš„å‰¯æœ¬ï¼Œä»¥ä¾¿åœ¨è¿­ä»£æ—¶ä¿®æ”¹åŸå§‹åˆ—è¡¨
    for handler in logger_obj.handlers[:]:
        logger_obj.removeHandler(handler)

    # 1. é…ç½®æ–‡ä»¶å¤„ç†å™¨ (FileHandler)
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setLevel(file_output_level) # è®¾ç½®æ–‡ä»¶å¤„ç†å™¨çº§åˆ«
    # å®šä¹‰æ–‡ä»¶æ—¥å¿—çš„æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    file_handler.setFormatter(formatter)
    logger_obj.addHandler(file_handler) # å°†æ–‡ä»¶å¤„ç†å™¨æ·»åŠ åˆ°æ—¥å¿—å™¨

    # 2. é…ç½®æ§åˆ¶å°å¤„ç†å™¨ (StreamHandler)
    console_handler = logging.StreamHandler(sys.stdout) # è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºæµ (æ§åˆ¶å°)
    console_handler.setLevel(console_output_level) # è®¾ç½®æ§åˆ¶å°å¤„ç†å™¨çº§åˆ«
    console_handler.setFormatter(formatter) # æ§åˆ¶å°ä¹Ÿä½¿ç”¨ç›¸åŒçš„æ ¼å¼
    logger_obj.addHandler(console_handler) # å°†æ§åˆ¶å°å¤„ç†å™¨æ·»åŠ åˆ°æ—¥å¿—å™¨

    return logger_obj

# ===== å•ä¸ªç‚¹ä½å¤„ç†å‡½æ•° (ç”¨äºå¤šçº¿ç¨‹) =====
def process_single_point(point_data_tuple, pano_id_str, api_key_str, session_token_str, zoom_int, tile_cols_int, tile_rows_int, tile_size_int, sleeptime_float, save_dir_str, logger_obj, thread_local_storage):
    """
    å¤„ç†å•ä¸ªç‚¹ä½çš„å›¾åƒä¸‹è½½å’Œæ‹¼æ¥ã€‚
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨ä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œè´Ÿè´£è·å–ç“¦ç‰‡å¹¶å°†å…¶æ‹¼æ¥åˆ°å…¨æ™¯å›¾ä¸­ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å¤„ç†ç»“æœçš„å­—å…¸ï¼ŒåŒ…å«æ›´è¯¦ç»†çš„é”™è¯¯ç±»å‹ã€‚
    """
    current_point_id_str = str(point_data_tuple.ID) # ä» namedtuple è·å–ç‚¹ä½ ID

    logger_obj.info(f"çº¿ç¨‹ {threading.get_ident()}: å¼€å§‹å¤„ç†ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str}")

    # å¦‚æœ pano_id_str ä¸ºç©ºæˆ– Noneï¼Œç›´æ¥è¿”å›â€œæœªæ‰¾åˆ° PanoIDâ€çš„å¤±è´¥çŠ¶æ€
    # è¿™é€šå¸¸å‘ç”Ÿåœ¨ PanoIds API è¿”å›ç©ºå­—ç¬¦ä¸²æ—¶ï¼Œè¡¨ç¤ºè¯¥åæ ‡æ²¡æœ‰è¡—æ™¯
    if not pano_id_str: 
        logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str} æœªæ‰¾åˆ° PanoIDã€‚")
        return {"status": "failure", "id": current_point_id_str, "reason": "No panoId found for this location", "error_type": ERROR_TYPE_NO_PANOID_FOUND}

    # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒï¼Œç”¨äºåç»­æ‹¼æ¥ç“¦ç‰‡
    panorama = Image.new('RGB', (tile_size_int * tile_cols_int, tile_size_int * tile_rows_int))
    missing_tiles_count = 0 # è®°å½•ç¼ºå¤±ç“¦ç‰‡çš„æ•°é‡
    total_tiles = tile_cols_int * tile_rows_int # æ€»ç“¦ç‰‡æ•°
    logger_obj.debug(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - åˆ›å»ºç©ºç™½å›¾åƒï¼Œé¢„æœŸæ€»ç“¦ç‰‡æ•°: {total_tiles}")

    # éå†æ‰€æœ‰ç“¦ç‰‡åæ ‡ (x, y)
    # trange æä¾›è¿›åº¦æ¡ï¼Œposition ç”¨äºåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹åˆ†æ•£è¿›åº¦æ¡ï¼Œé¿å…äº¤é”™
    for x in trange(tile_cols_int, desc=f"æ‹¼æ¥ {current_point_id_str} (çº¿ç¨‹ {threading.get_ident()})", leave=False, position=threading.get_ident() % 10):
        for y in range(tile_rows_int):
            # æ„å»ºç“¦ç‰‡ä¸‹è½½çš„ URL
            tile_url = (
                f"https://tile.googleapis.com/v1/streetview/tiles/{zoom_int}/{x}/{y}"
                f"?session={session_token_str}&key={api_key_str}&panoId={pano_id_str}"
            )
            logger_obj.debug(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - è¯·æ±‚ç“¦ç‰‡ URL: {tile_url}")

            max_tile_retries = 3 # æ¯ä¸ªç“¦ç‰‡çš„ä¸‹è½½å°è¯•æ¬¡æ•°
            current_tile_retry = 0 # å½“å‰ç“¦ç‰‡çš„é‡è¯•è®¡æ•°
            tile_download_successful = False # æ ‡è®°ç“¦ç‰‡æ˜¯å¦æˆåŠŸä¸‹è½½
            
            current_tile_error_reason = "" # è®°å½•å½“å‰ç“¦ç‰‡å¤±è´¥çš„è¯¦ç»†åŸå› 
            current_tile_error_type = "" # è®°å½•å½“å‰ç“¦ç‰‡å¤±è´¥çš„ç±»å‹

            # ç“¦ç‰‡ä¸‹è½½çš„å†…éƒ¨é‡è¯•å¾ªç¯
            while current_tile_retry < max_tile_retries:
                try:
                    # å‘é€ GET è¯·æ±‚ä¸‹è½½ç“¦ç‰‡ï¼Œè®¾ç½®è¶…æ—¶
                    tile_resp = requests.get(tile_url, timeout=10) 
                    if tile_resp.status_code == 200:
                        # æˆåŠŸä¸‹è½½ï¼Œæ‰“å¼€å›¾åƒå¹¶ç²˜è´´åˆ°å…¨æ™¯å›¾ä¸­
                        tile_img = Image.open(BytesIO(tile_resp.content))
                        panorama.paste(tile_img, (x * tile_size_int, y * tile_size_int))
                        tile_download_successful = True
                        logger_obj.debug(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - ç“¦ç‰‡ ({x},{y}) ä¸‹è½½æˆåŠŸã€‚")
                        break # æˆåŠŸï¼Œè·³å‡ºç“¦ç‰‡å†…éƒ¨é‡è¯•å¾ªç¯
                    else:
                        # HTTP çŠ¶æ€ç é 200ï¼Œå°è¯•è§£æ API è¿”å›çš„ JSON é”™è¯¯ä¿¡æ¯
                        error_detail = ""
                        try:
                            error_json = tile_resp.json()
                            error_detail = error_json.get("error", {}).get("message", "") or error_json.get("message", "")
                        except json.JSONDecodeError:
                            error_detail = tile_resp.text # å¦‚æœä¸æ˜¯ JSON å“åº”ï¼Œåˆ™ä½¿ç”¨åŸå§‹æ–‡æœ¬

                        current_tile_error_reason = f"ç“¦ç‰‡ ({x},{y}) HTTP {tile_resp.status_code}: {error_detail[:150]}..." # æˆªæ–­æ¶ˆæ¯ä»¥é˜²è¿‡é•¿
                        
                        # æ ¹æ® HTTP çŠ¶æ€ç ç²¾ç»†åˆ¤æ–­é”™è¯¯ç±»å‹
                        if tile_resp.status_code == 404: # ç“¦ç‰‡ä¸å­˜åœ¨
                            current_tile_error_type = ERROR_TYPE_ALL_TILES_MISSING # æ ‡è®°ä¸ºç“¦ç‰‡ç¼ºå¤±ç±»å‹
                            break # å¯¹äº 404ï¼Œé€šå¸¸ä¸å€¼å¾—ç“¦ç‰‡å†…éƒ¨é‡è¯•ï¼Œç«‹å³ç»“æŸå†…éƒ¨å¾ªç¯
                        elif tile_resp.status_code == 401 or tile_resp.status_code == 403: # è®¤è¯å¤±è´¥æˆ–æƒé™é—®é¢˜
                            current_tile_error_type = ERROR_TYPE_API_AUTH_FORBIDDEN # æ ‡è®°ä¸ºè‡´å‘½é”™è¯¯
                            break # è‡´å‘½é”™è¯¯ä¸å€¼å¾—ç“¦ç‰‡å†…éƒ¨é‡è¯•ï¼Œç«‹å³ç»“æŸå†…éƒ¨å¾ªç¯
                        elif tile_resp.status_code == 429: # é€Ÿç‡é™åˆ¶
                            current_tile_error_type = ERROR_TYPE_API_RATE_LIMIT
                            if current_tile_retry == max_tile_retries - 1: # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                break # é€€å‡ºç“¦ç‰‡å†…éƒ¨é‡è¯•
                        elif 400 <= tile_resp.status_code < 500: # å…¶ä»–å®¢æˆ·ç«¯é”™è¯¯ (å¦‚400 Bad Request)
                            current_tile_error_type = ERROR_TYPE_API_BAD_REQUEST # é€šå¸¸ä¸å€¼å¾—ç“¦ç‰‡å†…éƒ¨é‡è¯•
                            break
                        elif 500 <= tile_resp.status_code < 600: # æœåŠ¡å™¨é”™è¯¯
                            current_tile_error_type = ERROR_TYPE_API_SERVER_ERROR # å€¼å¾—é‡è¯•
                            if current_tile_retry == max_tile_retries - 1: # è¾¾åˆ°æœ€å¤§é‡è¯•
                                break
                        else: # æœªçŸ¥ HTTP çŠ¶æ€ç 
                            current_tile_error_type = ERROR_TYPE_UNCLASSIFIED_HTTP_STATUS
                            if current_tile_retry == max_tile_retries - 1:
                                break
                # æ•è·ç½‘ç»œè¯·æ±‚å¼‚å¸¸
                except requests.exceptions.Timeout as req_e:
                    current_tile_error_reason = f"ç“¦ç‰‡ ({x},{y}) è¯·æ±‚è¶…æ—¶: {req_e}"
                    current_tile_error_type = ERROR_TYPE_NETWORK_TIMEOUT
                except requests.exceptions.ConnectionError as req_e:
                    current_tile_error_reason = f"ç“¦ç‰‡ ({x},{y}) è¿æ¥é”™è¯¯: {req_e}"
                    current_tile_error_type = ERROR_TYPE_NETWORK_CONNECTION_ERROR
                except requests.exceptions.RequestException as req_e: # æ•è·å…¶ä»– requests å¼‚å¸¸
                    current_tile_error_reason = f"ç“¦ç‰‡ ({x},{y}) æœªçŸ¥è¯·æ±‚å¼‚å¸¸: {req_e}"
                    current_tile_error_type = ERROR_TYPE_UNCLASSIFIED_REQUEST_ERROR
                except Exception as e: # æ•è·å…¶ä»–é€šç”¨å¼‚å¸¸ï¼Œå¦‚ PIL å›¾åƒå¤„ç†é”™è¯¯
                    current_tile_error_reason = f"å¤„ç†ç“¦ç‰‡ ({x},{y}) æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"
                    current_tile_error_type = ERROR_TYPE_INTERNAL_PROCESSING_ERROR
                    tile_download_successful = False # ç¡®ä¿æ ‡è®°ä¸ºå¤±è´¥
                    break # ç«‹å³é€€å‡ºç“¦ç‰‡å†…éƒ¨é‡è¯•å¾ªç¯ï¼Œå› ä¸ºå†…éƒ¨é”™è¯¯é€šå¸¸é‡è¯•æ— ç”¨

                # å¦‚æœç“¦ç‰‡ä¸‹è½½æœªæˆåŠŸï¼Œä¸”å½“å‰é”™è¯¯ç±»å‹å…è®¸å†…éƒ¨é‡è¯•ï¼Œåˆ™è®°å½•è­¦å‘Šå¹¶è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
                if not tile_download_successful and current_tile_retry < max_tile_retries:
                    logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - {current_tile_error_reason}. ç±»å‹: {current_tile_error_type}. é‡è¯• {current_tile_retry + 1}/{max_tile_retries}")
                
                current_tile_retry += 1
                # å¦‚æœè¿˜æœªè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿›è¡Œç­‰å¾…ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                if current_tile_retry < max_tile_retries:
                    # å¯¹äº 429 é”™è¯¯ï¼Œç­‰å¾…æ—¶é—´ä¼šæ›´é•¿
                    if current_tile_error_type == ERROR_TYPE_API_RATE_LIMIT:
                        sleep_time_retry = min(sleeptime_float * (2 ** current_tile_retry) * 5, 60) 
                    else:
                        sleep_time_retry = min(sleeptime_float * (2 ** current_tile_retry), 30) # æ™®é€šæŒ‡æ•°é€€é¿ï¼Œæœ€å¤§ 30 ç§’
                    sleep(sleep_time_retry)
                else: # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡ºå¾ªç¯
                    break
            
            # å¦‚æœç“¦ç‰‡åœ¨å¤šæ¬¡å†…éƒ¨é‡è¯•åä»æœªæˆåŠŸä¸‹è½½
            if not tile_download_successful:
                missing_tiles_count += 1
                logger_obj.error(f"çº¿ç¨‹ {threading.get_ident()}: ID: {current_point_id_str}, PanoID: {pano_id_str} - ç“¦ç‰‡ ({x},{y}) æœ€ç»ˆä¸‹è½½å¤±è´¥ã€‚åŸå› : {current_tile_error_reason}, ç±»å‹: {current_tile_error_type}")
                
                # å¦‚æœé‡åˆ°è¢«è®¤ä¸ºæ˜¯è‡´å‘½çš„é”™è¯¯ç±»å‹ï¼Œç«‹å³è¿”å›ç‚¹ä½å¤±è´¥ï¼Œä¸å†å°è¯•å…¶ä»–ç“¦ç‰‡
                if current_tile_error_type in {
                    ERROR_TYPE_API_AUTH_FORBIDDEN, # æƒé™/è®¤è¯é—®é¢˜
                    ERROR_TYPE_API_BAD_REQUEST, # è¯·æ±‚å‚æ•°é”™è¯¯
                    ERROR_TYPE_INTERNAL_PROCESSING_ERROR, # å†…éƒ¨å¤„ç†é”™è¯¯
                    ERROR_TYPE_UNCLASSIFIED_HTTP_STATUS, # æœªçŸ¥ HTTP çŠ¶æ€ç 
                    ERROR_TYPE_UNCLASSIFIED_REQUEST_ERROR # æœªçŸ¥è¯·æ±‚å¼‚å¸¸
                }:
                    return {"status": "failure", "id": current_point_id_str, "reason": current_tile_error_reason, "error_type": current_tile_error_type}


            sleep(sleeptime_float) # æ¯æ¬¡ç“¦ç‰‡ä¸‹è½½åçš„å›ºå®šé—´éš”

    # å¦‚æœæ‰€æœ‰ç“¦ç‰‡éƒ½ç¼ºå¤±ï¼ˆå¹¶ä¸”åœ¨ç“¦ç‰‡ä¸‹è½½å¾ªç¯ä¸­æ²¡æœ‰æå‰è¿”å›è‡´å‘½é”™è¯¯ï¼‰
    if missing_tiles_count == total_tiles:
        logger_obj.warning(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - æ‰€æœ‰ç“¦ç‰‡å‡ç¼ºå¤±ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        # è¿™é€šå¸¸è¡¨ç¤º PanoId æ— æ•ˆï¼ˆå³ä½¿ä¸æ˜¯ Noneï¼Œç“¦ç‰‡æœ¬èº«ä¹Ÿè¿”å› 404ï¼‰æˆ–è€…æŒç»­çš„ç½‘ç»œ/API é—®é¢˜
        return {"status": "failure", "id": current_point_id_str, "reason": "All tiles missing after repeated attempts", "error_type": ERROR_TYPE_ALL_TILES_MISSING}

    # æˆåŠŸæ‹¼æ¥æ‰€æœ‰ç“¦ç‰‡ï¼Œå°è¯•ä¿å­˜å›¾åƒ
    filename = f"{current_point_id_str}_{pano_id_str}.jpg"
    filepath = os.path.join(save_dir_str, filename)
    try:
        panorama.save(filepath)
        logger_obj.info(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - å›¾åƒæˆåŠŸä¿å­˜è‡³: {filepath}")
        return {"status": "success", "id": current_point_id_str, "panoId": pano_id_str, "file": filename}
    except Exception as e_save: # æ•è·ä¿å­˜å›¾åƒæ—¶å¯èƒ½å‘ç”Ÿçš„å¼‚å¸¸
        logger_obj.error(f"çº¿ç¨‹ {threading.get_ident()}: ç‚¹ä½ ID: {current_point_id_str}, PanoID: {pano_id_str} - ä¿å­˜å›¾åƒå¤±è´¥: {e_save}", exc_info=True)
        return {"status": "failure", "id": current_point_id_str, "reason": f"Save image failed: {e_save}", "error_type": ERROR_TYPE_INTERNAL_PROCESSING_ERROR}


if __name__ == "__main__":
    # --- é˜¶æ®µ1: ç¨‹åºå¯åŠ¨æ—¶çš„åŸºç¡€æ—¥å¿—é…ç½® ---
    # åœ¨ä¸»ç¨‹åºçš„æœ€æ—©é˜¶æ®µè®¾ç½®ä¸€ä¸ªä¸´æ—¶çš„ã€ä»…è¾“å‡ºåˆ°æ§åˆ¶å°çš„æ—¥å¿—å™¨ã€‚
    # è¿™ç¡®ä¿äº†å³ä½¿åœ¨è¯»å–é…ç½®æ–‡ä»¶ç­‰æ—©æœŸæ­¥éª¤ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿæœ‰æ—¥å¿—è¾“å‡ºã€‚
    # å®ƒçš„çº§åˆ«è®¾ç½®ä¸º INFOï¼Œä»¥ä¾¿æ•è·ç¨‹åºå¯åŠ¨æ—¶çš„åŸºæœ¬ä¿¡æ¯å’Œé”™è¯¯ã€‚
    temp_console_handler = logging.StreamHandler(sys.stdout)
    temp_console_handler.setLevel(logging.INFO) # ä¸´æ—¶æ—¥å¿—å™¨è¾“å‡ºINFOåŠä»¥ä¸Šçº§åˆ«
    temp_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    temp_console_handler.setFormatter(temp_formatter)

    # è·å–ä¸€ä¸ªåä¸º 'main_process_logger' çš„æ—¥å¿—å™¨å®ä¾‹ã€‚
    # å®ƒçš„çº§åˆ«è®¾ç½®ä¸º INFOï¼Œç¡®ä¿èƒ½æ•è·åˆ°åŸºæœ¬ä¿¡æ¯ã€‚
    logger = logging.getLogger('main_process_logger')
    logger.setLevel(logging.INFO)
    # æ·»åŠ ä¸´æ—¶æ§åˆ¶å°å¤„ç†å™¨ã€‚
    logger.addHandler(temp_console_handler)
    # è®¾ç½® propagate ä¸º Falseï¼Œé˜²æ­¢æ—¥å¿—æ¶ˆæ¯ä¼ é€’ç»™æ ¹æ—¥å¿—å™¨ï¼Œé¿å…é‡å¤è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚
    logger.propagate = False 

    # tqdm å…¨å±€é”ï¼Œç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å®‰å…¨è¾“å‡º
    tqdm.set_lock(threading.RLock())
    thread_local_storage = threading.local() # ç”¨äºçº¿ç¨‹ç‰¹å®šçš„æ•°æ®ï¼Œå¦‚æœéœ€è¦çš„è¯

    try:
        print("ç¨‹åºå¯åŠ¨...") # ä½¿ç”¨ print ç¡®ä¿è¿™æ¡æ¶ˆæ¯æ€»èƒ½è¢«ç”¨æˆ·çœ‹åˆ°
        logger.info("ç¨‹åºå¯åŠ¨ï¼Œæ­£åœ¨è¯»å–é…ç½®æ–‡ä»¶ã€‚") # è¿™æ¡ä¼šè¿›å…¥ä¸´æ—¶æ—¥å¿—å™¨

        config = ConfigParser()
        if not os.path.exists('configuration.ini'):
            print("âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ 'configuration.ini' æœªæ‰¾åˆ°ã€‚è¯·åˆ›å»ºè¯¥æ–‡ä»¶ã€‚")
            logger.error("é…ç½®æ–‡ä»¶ 'configuration.ini' æœªæ‰¾åˆ°ï¼Œç¨‹åºå°†é€€å‡ºã€‚") # è¿™æ¡ä¼šè¿›å…¥ä¸´æ—¶æ—¥å¿—å™¨
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        with open('configuration.ini', 'r', encoding='utf-8') as f:
            config.read_file(f)

        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰è·¯å¾„
        CSV_PATH = config['PATHS']['CSV_PATH']
        API_KEY_PATH = config['PATHS']['API_KEY_PATH']
        SAVE_DIR = config['PATHS']['SAVE_DIR']
        LOG_PATH = config['PATHS']['LOG_PATH']
        FAIL_LOG_PATH = config['PATHS']['FAIL_LOG_PATH']
        DETAILED_LOG_PATH = config['PATHS']['DETAILED_LOG_PATH']

        # --- é˜¶æ®µ2: é…ç½®è¯¦ç»†æ—¥å¿—å™¨ ---
        # åœ¨è¯»å–å®Œæ‰€æœ‰é…ç½®è·¯å¾„åï¼Œé‡æ–°é…ç½®æ—¥å¿—å™¨åˆ°æ–‡ä»¶ï¼Œå¹¶è°ƒæ•´æ§åˆ¶å°è¾“å‡ºçº§åˆ«ã€‚
        try:
            # é¦–å…ˆç§»é™¤ä¹‹å‰æ·»åŠ çš„ä¸´æ—¶æ§åˆ¶å°å¤„ç†å™¨ï¼Œå› ä¸ºæˆ‘ä»¬å°†é…ç½®ä¸€ä¸ªæ›´ç²¾ç»†çš„æ—¥å¿—å™¨
            logger.removeHandler(temp_console_handler)
            # è°ƒç”¨ setup_logger é…ç½®è¯¦ç»†æ—¥å¿—å™¨ï¼Œæ–‡ä»¶è¾“å‡ºDEBUGï¼Œæ§åˆ¶å°è¾“å‡ºWARNING
            logger = setup_logger(DETAILED_LOG_PATH, 
                                  console_output_level=logging.WARNING, 
                                  file_output_level=logging.DEBUG)
            
            logger.info(f"æ—¥å¿—å·²é‡å®šå‘åˆ°æ–‡ä»¶: {DETAILED_LOG_PATH}") # è¿™æ¡ä¿¡æ¯ä¼šè¿›å…¥æ–‡ä»¶ (DEBUGçº§åˆ«)ï¼Œä½†ä¸ä¼šè¿›å…¥æ§åˆ¶å° (INFO < WARNING)
            print(f"æ—¥å¿—è¾“å‡ºå·²é…ç½®ï¼šæ§åˆ¶å°è¾“å‡ºWARNINGåŠä»¥ä¸Šï¼Œæ‰€æœ‰æ—¥å¿—è®°å½•åœ¨æ–‡ä»¶: {DETAILED_LOG_PATH}") # ä½¿ç”¨ print ç¡®ä¿ç”¨æˆ·çœ‹åˆ°é…ç½®ä¿¡æ¯
        except Exception as e_setup_logger:
            # å¦‚æœé…ç½®è¯¦ç»†æ—¥å¿—æ–‡ä»¶å¤±è´¥ï¼Œåˆ™è®°å½•é”™è¯¯ï¼Œå¹¶ç¨‹åºå°†ç»§ç»­ä½¿ç”¨æœ€åˆçš„åŸºç¡€æ—¥å¿—å™¨ (è¾“å‡ºåˆ°æ§åˆ¶å°)
            # temp_console_handler ä»ç„¶æœ‰æ•ˆï¼Œå› ä¸ºå®ƒæ²¡æœ‰è¢«å…³é—­ï¼Œåªæ˜¯ä» 'logger' ç§»é™¤äº†
            # åœ¨è¿™ç§å¼‚å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°å°† temp_console_handler æ·»åŠ å› loggerï¼Œæˆ–è€…ç®€å•åœ°è®© logger ä¿æŒå…¶åŸå§‹çŠ¶æ€
            # è¿™é‡Œï¼Œæˆ‘ä»¬è®© logger ä¿æŒåŸå§‹çŠ¶æ€ï¼ˆå³ main_process_logger ä»ç„¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰ï¼Œå¹¶è®°å½•é”™è¯¯
            logger.error(f"é…ç½®è¯¦ç»†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e_setup_logger}ã€‚æ—¥å¿—å°†ç»§ç»­è¾“å‡ºåˆ°æ§åˆ¶å°ã€‚", exc_info=True)
            # æ³¨æ„ï¼šæ­¤æ—¶ logger å·²ç»æ˜¯ 'main_process_logger'ï¼Œå®ƒä»ç„¶æœ‰å…¶åŸå§‹çš„ StreamHandler
            # æ‰€ä»¥åé¢çš„ logger.info ä»ç„¶ä¼šæ‰“å°åˆ°æ§åˆ¶å°ï¼Œè¿™ç¬¦åˆâ€œå›é€€â€è¡Œä¸º

        logger.info("ç¨‹åºå¼€å§‹è¿è¡Œã€‚") # è¿™æ¡ INFO æ¶ˆæ¯å°†åªå†™å…¥æ–‡ä»¶ï¼Œä¸ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°
        logger.info(f"é…ç½®æ–‡ä»¶ 'configuration.ini' åŠ è½½æˆåŠŸã€‚") # è¿™æ¡ INFO æ¶ˆæ¯ä¹Ÿå°†åªå†™å…¥æ–‡ä»¶

        # è¯»å–å…¶ä»–å‚æ•°é…ç½®
        BATCH_SIZE = int(config['PARAMS']['BATCH_SIZE'])
        NUM_BATCHES = int(config['PARAMS']['NUM_BATCHES'])
        RETRY_FAILED_POINTS = config.getboolean('PARAMS', 'RETRY_FAILED_POINTS', fallback=False)
        MAX_POINT_WORKERS = config.getint('PARAMS', 'MAX_POINT_WORKERS', fallback=5)
        logger.info(f"å‚æ•°åŠ è½½ï¼šBATCH_SIZE={BATCH_SIZE}, NUM_BATCHES={NUM_BATCHES}, RETRY_FAILED_POINTS={RETRY_FAILED_POINTS}, MAX_POINT_WORKERS={MAX_POINT_WORKERS}")

        ZOOM = int(config['TILES']['ZOOM'])
        TILE_SIZE = int(config['TILES']['TILE_SIZE'])
        TILE_COLS = int(config['TILES']['TILE_COLS'])
        TILE_ROWS = int(config['TILES']['TILE_ROWS'])
        SLEEPTIME = float(config['TILES']['SLEEPTIME'])
        logger.info(f"å‚æ•°åŠ è½½ï¼šZOOM={ZOOM}, TILE_SIZE={TILE_SIZE}, TILE_COLS={TILE_COLS}, TILE_ROWS={TILE_ROWS}, SLEEPTIME={SLEEPTIME}")

        print(f"ğŸ”§ å½“å‰è®¾ç½®ï¼šZOOM={ZOOM}, TILE_SIZE={TILE_SIZE}, TILE_COLS={TILE_COLS}, TILE_ROWS={TILE_ROWS}")
        print(f"ğŸ–¼ï¸ é¢„æœŸæ‹¼æ¥å›¾åƒå¤§å°ï¼š{TILE_COLS * TILE_SIZE} x {TILE_ROWS * TILE_SIZE} åƒç´ ")
        print(f"ğŸ”„ é‡è¯•å¤±è´¥ç‚¹ä½æ¨¡å¼: {'å¼€å¯' if RETRY_FAILED_POINTS else 'å…³é—­'}")
        print(f"ğŸ§µ ç‚¹ä½å¤„ç†å¹¶å‘çº¿ç¨‹æ•°: {MAX_POINT_WORKERS}")
        logger.info(f"ç‚¹ä½å¤„ç†å¹¶å‘çº¿ç¨‹æ•°: {MAX_POINT_WORKERS}")


        # åˆ›å»ºä¿å­˜å›¾åƒçš„ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨
        os.makedirs(SAVE_DIR, exist_ok=True)
        logger.info(f"ä¿å­˜ç›®å½• '{SAVE_DIR}' å·²ç¡®è®¤/åˆ›å»ºã€‚")
        
        # è¯»å– API Key
        if not os.path.exists(API_KEY_PATH):
            logger.error(f"API Key æ–‡ä»¶ '{API_KEY_PATH}' æœªæ‰¾åˆ°ã€‚")
            print(f"âŒ é”™è¯¯ï¼šAPI Key æ–‡ä»¶ '{API_KEY_PATH}' æœªæ‰¾åˆ°ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        with open(API_KEY_PATH, 'r') as f:
            API_KEY = f.readline().strip()
        logger.info(f"API Key ä» '{API_KEY_PATH}' åŠ è½½æˆåŠŸã€‚")

        # åŠ è½½å·²æˆåŠŸä¸‹è½½çš„IDï¼Œç”¨äºè·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
        downloaded_ids = set()
        if os.path.exists(LOG_PATH):
            try:
                log_content_df = pd.read_csv(LOG_PATH)
                if 'ID' in log_content_df.columns:
                    downloaded_ids = set(log_content_df['ID'].dropna().astype(str))
                    logger.info(f"ä» '{LOG_PATH}' åŠ è½½å·²æˆåŠŸä¸‹è½½ {len(downloaded_ids)} ä¸ªIDã€‚")
                else: logger.warning(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            except pd.errors.EmptyDataError: logger.warning(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' ä¸ºç©ºã€‚")
            except Exception as e_log: logger.error(f"è¯»å–æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_log}", exc_info=True)
        else:
            pd.DataFrame(columns=['ID']).to_csv(LOG_PATH, index=False)
            logger.info(f"'{LOG_PATH}' ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç©ºçš„æˆåŠŸæ—¥å¿—æ–‡ä»¶ã€‚")

        # failed_df_list ç”¨äºæ”¶é›†å½“å‰è¿è¡Œæ‰¹æ¬¡ä¸­äº§ç”Ÿçš„å¤±è´¥è®°å½•
        failed_df_list = []
        
        # åˆå§‹æ—¶ä»æ–‡ä»¶åŠ è½½å†å²å¤±è´¥è®°å½•ï¼Œç”¨äºæ„å»º ids_to_skip_processing é›†åˆ
        initial_failed_ids_from_file = set() # å­˜å‚¨æ‰€æœ‰å†å²å¤±è´¥çš„ID
        permanent_failed_ids_from_file = set() # å­˜å‚¨è¢«åˆ¤æ–­ä¸ºæ°¸ä¹…å¤±è´¥çš„ID (å³é‡è¯•æ¨¡å¼ä¸‹ä¹Ÿè¦è·³è¿‡çš„ID)

        if os.path.exists(FAIL_LOG_PATH):
            try:
                temp_failed_df = pd.read_csv(FAIL_LOG_PATH)
                if not temp_failed_df.empty and 'ID' in temp_failed_df.columns:
                    temp_failed_df['ID'] = temp_failed_df['ID'].astype(str)
                    initial_failed_ids_from_file = set(temp_failed_df['ID'].unique())

                    # æ£€æŸ¥å¤±è´¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦åŒ…å« 'error_type' åˆ—ï¼ˆç”¨äºå…¼å®¹æ—§æ ¼å¼ï¼‰
                    if 'error_type' in temp_failed_df.columns:
                        # å¦‚æœå­˜åœ¨ 'error_type' åˆ—ï¼Œåˆ™æ ¹æ®è¯¥åˆ—åˆ¤æ–­å“ªäº›æ˜¯æ°¸ä¹…æ€§å¤±è´¥
                        permanent_failed_df = temp_failed_df[temp_failed_df['error_type'].isin(PERMANENT_SKIP_ERROR_TYPES)]
                        permanent_failed_ids_from_file = set(permanent_failed_df['ID'].unique())
                        logger.info(f"ä» '{FAIL_LOG_PATH}' åˆå§‹åŠ è½½ {len(permanent_failed_ids_from_file)} ä¸ªå”¯ä¸€æ°¸ä¹…å¤±è´¥IDã€‚")
                    else:
                        # å¦‚æœç¼ºå°‘ 'error_type' åˆ—ï¼Œåˆ™å‘å‡ºè­¦å‘Šï¼Œå¹¶å°è¯•ä» 'Reason' åˆ—æ¨æ–­ 'NO_PANOID_FOUND'
                        logger.warning(f"å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' ä¸­ç¼ºå°‘ 'error_type' åˆ—ã€‚æ‰€æœ‰å†å²å¤±è´¥å°†è¢«è§†ä¸ºå¯é‡è¯•ï¼ˆé™¤äº†æ˜ç¡®çš„NO_PANOID_FOUNDï¼‰ã€‚")
                        no_panoid_from_reason = temp_failed_df[temp_failed_df['Reason'].str.contains("No panoId", na=False)]['ID'].unique()
                        permanent_failed_ids_from_file.update(no_panoid_from_reason)
                        logger.info(f"ä»æ—§å¤±è´¥æ—¥å¿—çš„ Reason æ¨æ–­å‡º {len(no_panoid_from_reason)} ä¸ª NO_PANOID_FOUNDã€‚")

                logger.info(f"ä» '{FAIL_LOG_PATH}' åˆå§‹åŠ è½½ {len(initial_failed_ids_from_file)} ä¸ªå”¯ä¸€å¤±è´¥IDã€‚")
            except pd.errors.EmptyDataError:
                logger.warning(f"å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' ä¸ºç©ºã€‚")
            except Exception as e_fail_log:
                logger.error(f"è¯»å–å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_fail_log}", exc_info=True)
        else:
            # å¦‚æœå¤±è´¥æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºæ–‡ä»¶ï¼Œå¹¶åŒ…å«æ‰€æœ‰åˆ—
            pd.DataFrame(columns=['ID', 'Reason', 'error_type']).to_csv(FAIL_LOG_PATH, index=False)
            logger.info(f"'{FAIL_LOG_PATH}' ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç©ºçš„å¤±è´¥æ—¥å¿—æ–‡ä»¶ã€‚")

        # æ„å»ºæœ€ç»ˆéœ€è¦è·³è¿‡çš„IDé›†åˆ
        # æ€»æ˜¯è·³è¿‡å·²æˆåŠŸä¸‹è½½çš„ID
        ids_to_skip_processing = set(downloaded_ids)
        if not RETRY_FAILED_POINTS:
            # å¦‚æœé‡è¯•æ¨¡å¼å…³é—­ï¼Œåˆ™é¢å¤–è·³è¿‡æ‰€æœ‰ä¹‹å‰è®°å½•çš„å¤±è´¥ID
            logger.info("é‡è¯•æ¨¡å¼å…³é—­ï¼Œå°†é¢å¤–è·³è¿‡æ‰€æœ‰ä¹‹å‰å¤±è´¥è®°å½•ä¸­çš„IDã€‚")
            ids_to_skip_processing.update(initial_failed_ids_from_file)
        else:
            # å¦‚æœé‡è¯•æ¨¡å¼å¼€å¯ï¼Œåˆ™åªè·³è¿‡å·²æˆåŠŸä¸‹è½½çš„ID å’Œ æ˜ç¡®çš„æ°¸ä¹…æ€§å¤±è´¥ID
            logger.info("é‡è¯•æ¨¡å¼å¼€å¯ï¼Œå°†ä»…è·³è¿‡å·²æˆåŠŸä¸‹è½½çš„IDï¼Œä»¥åŠç‰¹å®šæ°¸ä¹…å¤±è´¥çš„IDã€‚")
            ids_to_skip_processing.update(permanent_failed_ids_from_file)

        logger.info(f"æ€»è®¡å°†æ˜ç¡®è·³è¿‡ {len(ids_to_skip_processing)} ä¸ªIDã€‚")

        logger.info("å°è¯•åˆ›å»ºè¡—æ™¯ä¼šè¯ Token...")
        session_payload = {"mapType": "streetview", "language": "en-US", "region": "US"}
        try:
            # å‘é€è¯·æ±‚åˆ›å»ºä¼šè¯ Token
            session_response = requests.post(f"https://tile.googleapis.com/v1/createSession?key={API_KEY}", headers={"Content-Type": "application/json"}, json=session_payload, timeout=15)
            
            if session_response.status_code == 200:
                # æˆåŠŸè·å– Token
                SESSION_TOKEN = session_response.json().get("session")
                if not SESSION_TOKEN:
                    error_msg = f"æ— æ³•è·å– session tokenï¼Œå“åº”å†…å®¹ï¼š{session_response.text}"
                    logger.error(error_msg)
                    print(f"âŒ {error_msg}") # åœ¨æ§åˆ¶å°æ˜¾ç¤ºé”™è¯¯ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¯èƒ½å¯¼è‡´ç¨‹åºæ— æ³•ç»§ç»­çš„ä¸¥é‡é”™è¯¯
                    raise Exception("æ— æ³•è·å– session token")
                logger.info(f"æˆåŠŸè·å– Session Token: {SESSION_TOKEN[:10]}...")
            else:
                # Session Token è¯·æ±‚å¤±è´¥ï¼ˆé 200 çŠ¶æ€ç ï¼‰
                error_detail = ""
                try:
                    error_json = session_response.json()
                    error_detail = error_json.get("error", {}).get("message", "") or error_json.get("message", "")
                except json.JSONDecodeError:
                    error_detail = session_response.text

                error_reason = f"åˆ›å»º Session Token è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {session_response.status_code}, æ¶ˆæ¯: {error_detail[:200]}..."
                logger.error(error_reason) # è®°å½•åˆ°æ–‡ä»¶å’Œæ§åˆ¶å° (å› ä¸ºæ˜¯ ERROR çº§åˆ«)
                print(f"âŒ {error_reason}") # ç¡®ä¿åœ¨æ§åˆ¶å°æ˜¾ç¤ºç»™ç”¨æˆ·

                # æ ¹æ®çŠ¶æ€ç åˆ¤æ–­å¹¶æŠ›å‡ºç‰¹å®šå¼‚å¸¸
                if session_response.status_code == 401 or session_response.status_code == 403:
                    raise Exception(f"åˆ›å»º Session Token å¤±è´¥: API Key æ— æ•ˆæˆ–æƒé™ä¸è¶³ ({error_reason})")
                elif session_response.status_code == 429:
                    raise Exception(f"åˆ›å»º Session Token å¤±è´¥: é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯• ({error_reason})")
                elif session_response.status_code >= 500:
                    raise Exception(f"åˆ›å»º Session Token å¤±è´¥: æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œå¯èƒ½æš‚æ—¶æ€§ ({error_reason})")
                elif session_response.status_code == 400:
                    raise Exception(f"åˆ›å»º Session Token å¤±è´¥: è¯·æ±‚å‚æ•°é”™è¯¯ ({error_reason})")
                else:
                    raise Exception(f"åˆ›å»º Session Token å¤±è´¥: æœªçŸ¥HTTPçŠ¶æ€ç  ({error_reason})")
        except requests.exceptions.Timeout as req_e:
            logger.error(f"åˆ›å»º Session Token è¯·æ±‚è¶…æ—¶: {req_e}", exc_info=True)
            print(f"âŒ åˆ›å»º Session Token è¯·æ±‚è¶…æ—¶: {req_e}")
            raise Exception(f"åˆ›å»º Session Token è¯·æ±‚è¶…æ—¶: {req_e}")
        except requests.exceptions.ConnectionError as req_e:
            logger.error(f"åˆ›å»º Session Token è¿æ¥é”™è¯¯: {req_e}", exc_info=True)
            print(f"âŒ åˆ›å»º Session Token è¿æ¥é”™è¯¯: {req_e}")
            raise Exception(f"åˆ›å»º Session Token è¿æ¥é”™è¯¯: {req_e}")
        except requests.exceptions.RequestException as req_e:
            logger.error(f"åˆ›å»º Session Token è¯·æ±‚å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {req_e}", exc_info=True)
            print(f"âŒ åˆ›å»º Session Token è¯·æ±‚å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {req_e}")
            raise Exception(f"åˆ›å»º Session Token è¯·æ±‚å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {req_e}")
        except Exception as e_session_general:
            logger.error(f"åˆ›å»º Session Token å‘ç”Ÿæ„å¤–é”™è¯¯: {e_session_general}", exc_info=True)
            print(f"âŒ åˆ›å»º Session Token å‘ç”Ÿæ„å¤–é”™è¯¯: {e_session_general}")
            raise Exception(f"åˆ›å»º Session Token å‘ç”Ÿæ„å¤–é”™è¯¯: {e_session_general}")


        # æ£€æŸ¥ç‚¹ä½ CSV æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(CSV_PATH):
            logger.error(f"ç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' æœªæ‰¾åˆ°ã€‚")
            print(f"âŒ é”™è¯¯ï¼šç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' æœªæ‰¾åˆ°ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        all_df = pd.read_csv(CSV_PATH)
        if 'ID' not in all_df.columns:
            logger.error(f"ç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            print(f"âŒ é”™è¯¯ï¼šç‚¹ä½CSVæ–‡ä»¶ '{CSV_PATH}' ä¸­ç¼ºå°‘ 'ID' åˆ—ã€‚")
            input("\næŒ‰å›è½¦é”®å…³é—­...")
            exit()
        all_df['ID'] = all_df['ID'].astype(str) # ç¡®ä¿ ID åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
        logger.info(f"ä» '{CSV_PATH}' åŠ è½½ {len(all_df)} ä¸ªæ€»ç‚¹ä½ã€‚")
        
        current_run_log_list = [] # å­˜å‚¨å½“å‰è¿è¡Œä¸­æˆåŠŸä¸‹è½½çš„IDï¼Œç”¨äºæ›´æ–°æˆåŠŸæ—¥å¿—

        # éå†æ‰¹æ¬¡è¿›è¡Œå¤„ç†
        for batch_num in range(NUM_BATCHES):
            logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{NUM_BATCHES}") # è¿™æ¡ä¿¡æ¯ä¼šè¿›å…¥æ–‡ä»¶ï¼Œä¸ä¼šè¿›å…¥æ§åˆ¶å°
            
            # æ ¹æ®å·²è·³è¿‡IDé›†åˆç­›é€‰å½“å‰æ‰¹æ¬¡è¦å¤„ç†çš„ç‚¹ä½
            current_processing_df = all_df[~all_df['ID'].isin(list(ids_to_skip_processing))].head(BATCH_SIZE)
            
            if current_processing_df.empty:
                print("ğŸ‰ æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç‚¹ä½å·²å¤„ç†å®Œæ¯•ï¼Œæ— éœ€å†è¿è¡Œæ›´å¤šæ‰¹æ¬¡ã€‚") # ä½¿ç”¨ print ç¡®ä¿ç”¨æˆ·çœ‹åˆ°
                logger.info("æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç‚¹ä½å·²å¤„ç†å®Œæ¯•ï¼Œæ— éœ€å†è¿è¡Œæ›´å¤šæ‰¹æ¬¡ã€‚") # è¿™æ¡ä¿¡æ¯ä¼šè¿›å…¥æ–‡ä»¶
                break # æ‰€æœ‰ç‚¹ä½éƒ½å·²å¤„ç†ï¼Œé€€å‡ºæ‰¹æ¬¡å¾ªç¯
            
            print(f"\nğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {batch_num + 1}/{NUM_BATCHES} æ‰¹ï¼Œå…± {len(current_processing_df)} ä¸ªç‚¹ä½...") # ä½¿ç”¨ print ç¡®ä¿ç”¨æˆ·çœ‹åˆ°
            logger.info(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šå¾…å¤„ç†ç‚¹ä½æ•° {len(current_processing_df)}") # è¿™æ¡ä¿¡æ¯ä¼šè¿›å…¥æ–‡ä»¶

            # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ•°æ®æ˜¯å¦åŒ…å«ç»çº¬åº¦åˆ—
            if not all(col in current_processing_df.columns for col in ['Lat', 'Lng']):
                logger.error("å½“å‰æ‰¹æ¬¡ç‚¹ä½æ•°æ®ä¸­ç¼ºå°‘ 'Lat' æˆ– 'Lng' åˆ—ã€‚è¯·æ£€æŸ¥CSVæ–‡ä»¶ã€‚") # è®°å½•åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
                print("âŒ é”™è¯¯ï¼šå½“å‰æ‰¹æ¬¡ç‚¹ä½æ•°æ®ä¸­ç¼ºå°‘ 'Lat' æˆ– 'Lng' åˆ—ã€‚") # æ§åˆ¶å°è¾“å‡º
                continue # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡

            # å‡†å¤‡è¯·æ±‚ PanoIDs çš„åœ°ç†ä½ç½®åˆ—è¡¨
            locations = [{"lat": row["Lat"], "lng": row["Lng"]} for _, row in current_processing_df.iterrows()]
            logger.debug(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè¯·æ±‚ PanoIDs çš„åœ°ç‚¹ (å‰5ä¸ª): {locations[:5]}") # è¿™æ¡ä¿¡æ¯åªè¿›å…¥æ–‡ä»¶

            # è¯·æ±‚ PanoIDs
            panoid_url = f"https://tile.googleapis.com/v1/streetview/panoIds?session={SESSION_TOKEN}&key={API_KEY}"
            pano_ids_data = [] # å­˜å‚¨ PanoID å“åº”æ•°æ®
            try:
                response_pano_ids = requests.post(panoid_url, json={"locations": locations, "radius": 50}, timeout=20)
                if response_pano_ids.status_code == 200:
                    try:
                        pano_ids_data = response_pano_ids.json().get("panoIds", [])
                        logger.info(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šæˆåŠŸè·å– PanoIDs å“åº”ã€‚æ•°é‡: {len(pano_ids_data)}") # æ–‡ä»¶æ—¥å¿—
                        logger.debug(f"æ‰¹æ¬¡ {batch_num + 1}ï¼šè·å–åˆ°çš„ PanoIDs (éƒ¨åˆ†): {pano_ids_data[:5]}") # æ–‡ä»¶æ—¥å¿—
                    except json.JSONDecodeError: # JSON è§£æå¤±è´¥
                        error_reason = f"è·å– PanoIDs æˆåŠŸï¼Œä½†JSONè§£æå¤±è´¥ã€‚å“åº”æ–‡æœ¬: {response_pano_ids.text[:200]}..."
                        logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        # å°†æœ¬æ‰¹æ¬¡æ‰€æœ‰ç‚¹ä½æ ‡è®°ä¸ºå¤±è´¥ï¼Œå¹¶è®°å½• JSON è§£æé”™è¯¯ç±»å‹
                        for point_row_tuple in current_processing_df.itertuples(index=False):
                            failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": ERROR_TYPE_PANOID_JSON_PARSE_ERROR})
                            ids_to_skip_processing.add(str(point_row_tuple.ID)) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¤±è´¥ï¼‰
                        continue # è·³è¿‡å½“å‰æ‰¹æ¬¡çš„ç“¦ç‰‡ä¸‹è½½ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡
                else:
                    # PanoIDs è¯·æ±‚å¤±è´¥ï¼ˆé 200 çŠ¶æ€ç ï¼‰
                    error_detail = ""
                    try:
                        error_json = response_pano_ids.json()
                        error_detail = error_json.get("error", {}).get("message", "") or error_json.get("message", "")
                    except json.JSONDecodeError:
                        error_detail = response_pano_ids.text

                    error_reason = f"è·å– PanoIDs è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {response_pano_ids.status_code}, æ¶ˆæ¯: {error_detail[:200]}..."
                    logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                    
                    # æ ¹æ®çŠ¶æ€ç åˆ¤æ–­ PanoID è¯·æ±‚çš„é”™è¯¯ç±»å‹
                    current_batch_error_type = ERROR_TYPE_UNCLASSIFIED_HTTP_STATUS # é»˜è®¤æœªçŸ¥HTTPçŠ¶æ€ç 
                    if response_pano_ids.status_code == 400: # Bad Request
                        current_batch_error_type = ERROR_TYPE_API_BAD_REQUEST
                    elif response_pano_ids.status_code == 401 or response_pano_ids.status_code == 403: # Forbidden
                        current_batch_error_type = ERROR_TYPE_API_AUTH_FORBIDDEN
                    elif response_pano_ids.status_code == 429: # Too Many Requests
                        current_batch_error_type = ERROR_TYPE_API_RATE_LIMIT
                    elif 500 <= response_pano_ids.status_code < 600: # Server Error
                        current_batch_error_type = ERROR_TYPE_API_SERVER_ERROR

                    # å°†æœ¬æ‰¹æ¬¡æ‰€æœ‰ç‚¹ä½æ ‡è®°ä¸ºç›¸åº”çš„å¤±è´¥ç±»å‹
                    for point_row_tuple in current_processing_df.itertuples(index=False):
                        failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": current_batch_error_type})
                        ids_to_skip_processing.add(str(point_row_tuple.ID)) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¤±è´¥ï¼‰
                    continue # è·³è¿‡å½“å‰æ‰¹æ¬¡çš„ç“¦ç‰‡ä¸‹è½½ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡

            except requests.exceptions.Timeout as req_e_pano: # PanoIDs è¯·æ±‚è¶…æ—¶
                error_reason = f"è·å– PanoIDs è¯·æ±‚è¶…æ—¶: {req_e_pano}"
                logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                for point_row_tuple in current_processing_df.itertuples(index=False):
                    failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": ERROR_TYPE_NETWORK_TIMEOUT})
                    ids_to_skip_processing.add(str(point_row_tuple.ID))
                continue # è·³è¿‡å½“å‰æ‰¹æ¬¡çš„ç“¦ç‰‡ä¸‹è½½
            except requests.exceptions.ConnectionError as req_e_pano: # PanoIDs è¿æ¥é”™è¯¯
                error_reason = f"è·å– PanoIDs è¿æ¥é”™è¯¯: {req_e_pano}"
                logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                for point_row_tuple in current_processing_df.itertuples(index=False):
                    failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": ERROR_TYPE_NETWORK_CONNECTION_ERROR})
                    ids_to_skip_processing.add(str(point_row_tuple.ID))
                continue
            except requests.exceptions.RequestException as req_e_pano: # å…¶ä»– requests å¼‚å¸¸
                error_reason = f"è·å– PanoIDs è¯·æ±‚å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {req_e_pano}"
                logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                for point_row_tuple in current_processing_df.itertuples(index=False):
                    failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": ERROR_TYPE_UNCLASSIFIED_REQUEST_ERROR})
                    ids_to_skip_processing.add(str(point_row_tuple.ID))
                continue
            except Exception as e_pano_general: # æ•è·å…¶ä»–é€šç”¨å¼‚å¸¸
                error_reason = f"è·å– PanoIDs å‘ç”Ÿæ„å¤–é”™è¯¯: {e_pano_general}"
                logger.error(f"æ‰¹æ¬¡ {batch_num + 1}ï¼š{error_reason}", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿— (å«å †æ ˆä¿¡æ¯)
                for point_row_tuple in current_processing_df.itertuples(index=False):
                    failed_df_list.append({"ID": str(point_row_tuple.ID), "Reason": error_reason, "error_type": ERROR_TYPE_GENERAL_EXCEPTION}) # æ•è·é€šç”¨å¼‚å¸¸
                    ids_to_skip_processing.add(str(point_row_tuple.ID))
                continue
            
            print("ğŸ“ å·²è·å– panoIds") # ä½¿ç”¨ print
            results_this_batch_filenames = [] # å­˜å‚¨å½“å‰æ‰¹æ¬¡æˆåŠŸä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯

            # ç¡®ä¿ pano_ids_data é•¿åº¦ä¸ current_processing_df åŒ¹é…
            # Google API é€šå¸¸ä¼šè¿”å›ä¸è¯·æ±‚ locations æ•°é‡ç›¸åŒçš„ panoIds åˆ—è¡¨ï¼Œ
            # å³ä½¿æŸäº›åœ°ç‚¹æ²¡æœ‰ panoId ä¹Ÿä¼šç”¨ç©ºå­—ç¬¦ä¸² "" å¡«å……ã€‚
            # è¿™é‡Œæ˜¯ä¸ºäº†ä»¥é˜²ä¸‡ä¸€ API å“åº”é•¿åº¦ä¸åŒ¹é…ï¼Œç”¨ None è¡¥é½ã€‚
            if len(pano_ids_data) != len(locations):
                logger.warning(f"æ‰¹æ¬¡ {batch_num + 1}: PanoIDæ•°é‡({len(pano_ids_data)})ä¸åœ°ç‚¹æ•°({len(locations)})ä¸åŒ¹é…ã€‚å°†å°è¯•æŒ‰åœ°ç‚¹æ•°è¿­ä»£ï¼ŒPanoIDä¸è¶³å¤„ä¼šä¸ºNoneã€‚") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                pano_ids_data.extend([None] * (len(locations) - len(pano_ids_data)))


            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ªç‚¹ä½
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_POINT_WORKERS) as executor:
                future_to_point = {} # æ˜ å°„ Future å¯¹è±¡åˆ°ç‚¹ä½ ID
                for i, point_row_tuple in enumerate(current_processing_df.itertuples(index=False)):
                    current_pano_id = pano_ids_data[i] if i < len(pano_ids_data) else None
                    
                    # æäº¤å¤„ç†å•ä¸ªç‚¹ä½çš„ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                    future = executor.submit(process_single_point, 
                                            point_row_tuple, current_pano_id, API_KEY, SESSION_TOKEN, 
                                            ZOOM, TILE_COLS, TILE_ROWS, TILE_SIZE, 
                                            SLEEPTIME, SAVE_DIR, logger, thread_local_storage)
                    future_to_point[future] = str(point_row_tuple.ID) # ä½¿ç”¨åŸå§‹IDä½œä¸ºé”®

                # åŒ…è£… concurrent.futures.as_completedï¼Œä»¥ä¾¿æ˜¾ç¤ºæ€»ä½“è¿›åº¦æ¡
                for future in tqdm(concurrent.futures.as_completed(future_to_point), total=len(future_to_point), desc=f"å¤„ç†æ‰¹æ¬¡ {batch_num + 1} ç‚¹ä½"):
                    point_id_processed = future_to_point[future] # è·å–å·²å¤„ç†ç‚¹ä½çš„ ID
                    try:
                        result = future.result() # è·å–çº¿ç¨‹çš„è¿”å›ç»“æœ
                        if result['status'] == 'success':
                            # å¦‚æœå¤„ç†æˆåŠŸï¼Œè®°å½•åˆ°æˆåŠŸåˆ—è¡¨å’Œå·²è·³è¿‡IDé›†åˆ
                            results_this_batch_filenames.append({"ID": result['id'], "panoId": result['panoId'], "file": result['file']})
                            current_run_log_list.append({"ID": result['id']})
                            ids_to_skip_processing.add(result['id']) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆæˆåŠŸï¼‰
                        else: # status == 'failure'
                            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè®°å½•åˆ°å¤±è´¥åˆ—è¡¨ï¼Œå¹¶æ ‡è®°ä¸ºå·²å¤„ç†
                            reason = result.get('reason', 'æœªçŸ¥å¤±è´¥')
                            error_type = result.get('error_type', ERROR_TYPE_GENERAL_EXCEPTION) 
                            failed_df_list.append({"ID": result['id'], "Reason": reason, "error_type": error_type})
                            ids_to_skip_processing.add(result['id']) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¤±è´¥ï¼‰
                    except Exception as exc: # æ•è·çº¿ç¨‹æ‰§è¡Œè¿‡ç¨‹ä¸­æœªè¢« process_single_point æ•è·çš„å¼‚å¸¸
                        logger.error(f"ç‚¹ä½ID {point_id_processed} åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œæ—¶äº§ç”Ÿæœªæ•è·å¼‚å¸¸: {exc}", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        failed_df_list.append({"ID": point_id_processed, "Reason": f"çº¿ç¨‹ä¸­æœªæ•è·å¼‚å¸¸: {exc}", "error_type": ERROR_TYPE_GENERAL_EXCEPTION})
                        ids_to_skip_processing.add(point_id_processed) # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå¼‚å¸¸ï¼‰
            
            # ===== æ¯æ‰¹æ¬¡ç»“æŸæ—¶ä¿å­˜ä¸€æ¬¡æœ¬æ‰¹æ¬¡äº§ç”Ÿçš„å¤±è´¥è®°å½•å’Œæ‰¹æ¬¡ç»“æœ =====
            if failed_df_list: # å¦‚æœå½“å‰è¿è¡Œçš„æ‰¹æ¬¡ä¸­äº§ç”Ÿäº†å¤±è´¥è®°å½•
                temp_batch_failed_df = pd.DataFrame(failed_df_list)
                # è¯»å–æ—§çš„å¤±è´¥æ—¥å¿—ï¼Œåˆå¹¶å½“å‰æ‰¹æ¬¡äº§ç”Ÿçš„å¤±è´¥ï¼Œå»é‡åä¿å­˜
                if os.path.exists(FAIL_LOG_PATH):
                    try:
                        old_failed_df = pd.read_csv(FAIL_LOG_PATH)
                        # å¦‚æœæ—§æ—¥å¿—æ²¡æœ‰ 'error_type' åˆ—ï¼Œåˆ™æ·»åŠ å¹¶å¡«å……é»˜è®¤å€¼ï¼Œç¡®ä¿åˆå¹¶åçš„DataFrameç»“æ„ä¸€è‡´
                        if 'error_type' not in old_failed_df.columns:
                            old_failed_df['error_type'] = ERROR_TYPE_GENERAL_EXCEPTION # é»˜è®¤å€¼
                            logger.warning(f"æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' ç¼ºå°‘ 'error_type' åˆ—ï¼Œå·²æ·»åŠ å¹¶å¡«å……é»˜è®¤å€¼ã€‚") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        
                        if not old_failed_df.empty and 'ID' in old_failed_df.columns and 'Reason' in old_failed_df.columns:
                             combined_failed_df = pd.concat([old_failed_df, temp_batch_failed_df], ignore_index=True)
                        else: # æ—§æ—¥å¿—ä¸ºç©ºæˆ–æ ¼å¼ä¸å¯¹
                            combined_failed_df = temp_batch_failed_df
                    except pd.errors.EmptyDataError: # æ—§æ—¥å¿—æ–‡ä»¶ä¸ºç©º
                        combined_failed_df = temp_batch_failed_df
                    except Exception as e_read_old_fail: # è¯»å–æ—§æ—¥å¿—å¤±è´¥
                        logger.error(f"è¯»å–æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_read_old_fail}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        # print(f"âŒ è¯»å–æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_read_old_fail}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•ã€‚") # å¦‚æœå¸Œæœ›æ§åˆ¶å°é¢å¤–è¾“å‡º
                        combined_failed_df = temp_batch_failed_df
                else: # æ—§å¤±è´¥æ—¥å¿—ä¸å­˜åœ¨
                    combined_failed_df = temp_batch_failed_df
                
                # æ£€æŸ¥åˆå¹¶åçš„DataFrameæ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—ï¼Œå¹¶å»é‡ä¿å­˜
                if 'ID' in combined_failed_df.columns and 'Reason' in combined_failed_df.columns and 'error_type' in combined_failed_df.columns:
                    combined_failed_df['ID'] = combined_failed_df['ID'].astype(str)
                    # æ ¹æ® 'ID', 'Reason' å’Œ 'error_type' è¿›è¡Œå»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
                    combined_failed_df.drop_duplicates(subset=['ID', 'Reason', 'error_type'], keep='last').to_csv(FAIL_LOG_PATH, index=False)
                    logger.info(f"å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å·²æ›´æ–°ã€‚") # æ–‡ä»¶æ—¥å¿—
                else:
                    logger.error(f"æ— æ³•æ›´æ–°å¤±è´¥æ—¥å¿—ï¼Œå› ä¸ºåˆå¹¶åçš„æ—¥å¿—ç¼ºå°‘å¿…è¦çš„åˆ— (ID, Reason, æˆ– error_type)ã€‚") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                failed_df_list = [] # æ¸…ç©ºå½“å‰è¿è¡Œçš„å¤±è´¥åˆ—è¡¨ï¼Œé¿å…é‡å¤æ·»åŠ 

            # ä¿å­˜å½“å‰æ‰¹æ¬¡æˆåŠŸä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨
            if results_this_batch_filenames:
                pd.DataFrame(results_this_batch_filenames).to_csv(os.path.join(SAVE_DIR, f'results_batch_{batch_num+1}.csv'), index=False)
            logger.info(f"æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆã€‚å¤±è´¥æ—¥å¿—å’Œæ‰¹æ¬¡ç»“æœå·²æ›´æ–°ã€‚") # æ–‡ä»¶æ—¥å¿—

        # ===== æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œç»Ÿä¸€æ›´æ–°æˆåŠŸæ—¥å¿— =====
        if current_run_log_list:
            new_success_df = pd.DataFrame(current_run_log_list)
            if os.path.exists(LOG_PATH):
                try:
                    old_log_df = pd.read_csv(LOG_PATH)
                    if 'ID' not in old_log_df.columns: combined_log_df = new_success_df
                    else: combined_log_df = pd.concat([old_log_df, new_success_df], ignore_index=True)
                except pd.errors.EmptyDataError: combined_log_df = new_success_df
                except Exception as e_read_old_log:
                    logger.error(f"è¯»å–æ—§æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_read_old_log}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                    # print(f"âŒ è¯»å–æ—§æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_read_old_log}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•ã€‚") # å¦‚æœéœ€è¦æ§åˆ¶å°é¢å¤–è¾“å‡º
                    combined_log_df = new_success_df
            else: combined_log_df = new_success_df

            if 'ID' in combined_log_df.columns:
                combined_log_df['ID'] = combined_log_df['ID'].astype(str)
                combined_log_df.drop_duplicates(subset=['ID'], keep='last').to_csv(LOG_PATH, index=False)
                logger.info(f"æˆåŠŸæ—¥å¿— '{LOG_PATH}' å·²æ›´æ–°ã€‚") # æ–‡ä»¶æ—¥å¿—
            else: logger.error(f"æ— æ³•æ›´æ–°æˆåŠŸæ—¥å¿—ï¼Œå› ä¸ºåˆå¹¶åçš„æ—¥å¿—ç¼ºå°‘ 'ID' åˆ—ã€‚") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—

        print("\nâœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚") # å§‹ç»ˆåœ¨æ§åˆ¶å°æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        logger.info("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆã€‚") # æ–‡ä»¶æ—¥å¿—

    except Exception as e: # æ•è·ä¸»ç¨‹åºè¿è¡Œæ—¶çš„é¡¶å±‚æœªå¤„ç†å¼‚å¸¸
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}") # å§‹ç»ˆåœ¨æ§åˆ¶å°æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        # æ­¤æ—¶ logger å·²ç»ç¡®ä¿è¢«åˆå§‹åŒ–ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
        if logger: logger.error("ç¨‹åºé¡¶å±‚æ•è·åˆ°æœªå¤„ç†å¼‚å¸¸ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿— (å«å †æ ˆä¿¡æ¯)
        else: 
            # è¿™ç§æƒ…å†µæå°‘å‘ç”Ÿï¼Œæ„å‘³ç€åœ¨åŸºç¡€ logger åˆå§‹åŒ–ä¹‹å‰å°±å‘ç”Ÿäº†ä¸¥é‡é”™è¯¯
            print("Logger æœªåˆå§‹åŒ–ï¼Œè¯¦ç»†é”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š")
            traceback.print_exc() 

        try: # å³ä½¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ï¼Œä¹Ÿå°è¯•ä¿å­˜å·²æ”¶é›†çš„å¤±è´¥å’ŒæˆåŠŸè®°å½•ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
            if 'failed_df_list' in locals() and failed_df_list:
                temp_failed_df_on_exit = pd.DataFrame(failed_df_list)
                if os.path.exists(FAIL_LOG_PATH):
                    try:
                        old_failed_df = pd.read_csv(FAIL_LOG_PATH)
                        if 'error_type' not in old_failed_df.columns:
                            old_failed_df['error_type'] = ERROR_TYPE_GENERAL_EXCEPTION # é»˜è®¤å€¼
                            # æ­¤æ—¶ logger å¿…å®šä¸ä¸º Noneï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨
                            if logger: logger.warning(f"æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' ç¼ºå°‘ 'error_type' åˆ—ï¼Œå·²æ·»åŠ å¹¶å¡«å……é»˜è®¤å€¼ã€‚") # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        
                        if not old_failed_df.empty and 'ID' in old_failed_df.columns and 'Reason' in old_failed_df.columns:
                            combined_failed_df = pd.concat([old_failed_df, temp_failed_df_on_exit], ignore_index=True)
                        else:
                            combined_failed_df = temp_failed_df_on_exit
                    except pd.errors.EmptyDataError:
                        combined_failed_df = temp_failed_df_on_exit
                    except Exception as e_read_old_fail:
                        print(f"è¯»å–æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_read_old_fail}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•ã€‚") # æ§åˆ¶å°è¾“å‡º
                        if logger: logger.error(f"è¯»å–æ—§å¤±è´¥æ—¥å¿— '{FAIL_LOG_PATH}' å¤±è´¥: {e_read_old_fail}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„å¤±è´¥è®°å½•ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        combined_failed_df = temp_failed_df_on_exit
                else:
                    combined_failed_df = temp_failed_df_on_exit

                if 'ID' in combined_failed_df.columns and 'Reason' in combined_failed_df.columns and 'error_type' in combined_failed_df.columns:
                    combined_failed_df['ID'] = combined_failed_df['ID'].astype(str)
                    combined_failed_df.drop_duplicates(subset=['ID', 'Reason', 'error_type'], keep='last').to_csv(FAIL_LOG_PATH, index=False)
                    if logger:
                        logger.info("ç¨‹åºå¼‚å¸¸é€€å‡ºå‰ï¼Œå°è¯•ä¿å­˜å½“å‰æ”¶é›†çš„å¤±è´¥è®°å½•ã€‚") # æ–‡ä»¶æ—¥å¿—
                else:
                    print(f"âŒ åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜å¤±è´¥æ—¥å¿—å¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚") # æ§åˆ¶å°è¾“å‡º
                    if logger: logger.error(f"åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜å¤±è´¥æ—¥å¿—å¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—

            if 'current_run_log_list' in locals() and current_run_log_list:
                new_success_df = pd.DataFrame(current_run_log_list)
                if os.path.exists(LOG_PATH):
                    try:
                        old_log_df = pd.read_csv(LOG_PATH)
                        if 'ID' not in old_log_df.columns:
                            combined_log_df = new_success_df
                        else:
                            combined_log_df = pd.concat([old_log_df, new_success_df], ignore_index=True)
                    except pd.errors.EmptyDataError:
                        combined_log_df = new_success_df
                    except Exception as e_read_old_log:
                        print(f"è¯»å–æ—§æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_read_old_log}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•ã€‚") # æ§åˆ¶å°è¾“å‡º
                        if logger: logger.error(f"è¯»å–æ—§æˆåŠŸæ—¥å¿— '{LOG_PATH}' å¤±è´¥: {e_read_old_log}ã€‚å°†ä»…ä¿å­˜å½“å‰è¿è¡Œçš„æˆåŠŸè®°å½•ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
                        combined_log_df = new_success_df
                else:
                    combined_log_df = new_success_df

                if 'ID' in combined_log_df.columns:
                    combined_log_df['ID'] = combined_log_df['ID'].astype(str)
                    combined_log_df.drop_duplicates(subset=['ID'], keep='last').to_csv(LOG_PATH, index=False)
                    if logger:
                        logger.info("ç¨‹åºå¼‚å¸¸é€€å‡ºå‰ï¼Œå°è¯•ä¿å­˜å½“å‰æ”¶é›†çš„æˆåŠŸè®°å½•ã€‚") # æ–‡ä»¶æ—¥å¿—
                else:
                    print(f"âŒ åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æˆåŠŸæ—¥å¿—å¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚") # æ§åˆ¶å°è¾“å‡º
                    if logger: logger.error(f"åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æˆåŠŸæ—¥å¿—å¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—

        except Exception as log_save_e:
            print(f"âŒ åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æ—¥å¿—æ—¶ä¹Ÿå‘ç”Ÿé”™è¯¯: {log_save_e}") # æ§åˆ¶å°è¾“å‡º
            if logger: logger.error(f"åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¿å­˜æ—¥å¿—æ—¶ä¹Ÿå‘ç”Ÿé”™è¯¯: {log_save_e}", exc_info=True) # æ–‡ä»¶å’Œæ§åˆ¶å°æ—¥å¿—
    finally:
        # ç¨‹åºç»“æŸæ—¶ï¼Œå¦‚æœ logger å·²ç»åˆå§‹åŒ–ï¼Œåˆ™è®°å½•ç»“æŸä¿¡æ¯
        if logger: logger.info("ç¨‹åºç»“æŸã€‚\n---------------------------------------\n") # æ–‡ä»¶æ—¥å¿—
        input("\næŒ‰å›è½¦é”®å…³é—­...")